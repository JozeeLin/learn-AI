# 第二章 线性代数

线性代数主要是面向**连续数学**，而非离散数学。

如果你已经了解了线性代数的概念，但需要一份索引表来回顾一些重要公式，那么我们推荐《the matrix cookbook》

## 2.5 范数

范数用来衡量一个向量的大小。形式上，$L^P$范数定义如下：
$$
\|x\|_p = (\sum_{i} |x_i|^p)^{\frac{1}{p}} \tag{2.30}
$$
其中，$p \in \mathbb{R}, p \ge 1$。

范数(包括$L^p$范数)是将向量映射到非负值的函数。

更严格的说，范数是满足下列性质的任意函数:

- f(x)=0 => x=0
- $f(x+y) \le f(x)+f(y)$(三角不等式)
- $\forall \alpha \in \mathbb{R}, f(\alpha x)=|\alpha| f(x)$ 

常用范数的特性，以及使用场合：

1. 平方L2范数，在原点附近增长的十分缓慢
2. 在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素是很重要的。在这些情况下使用L1范数
3. 统计向量中非零元素的个数来衡量向量的大小。通常使用L1范数来表示
4. $L^{\infty}$，也称为最大范数(max norm)。表示向量中具有最大幅值的元素的绝对值:$\|x\|_{\infty} = \max_x |x_i|$

5. 衡量矩阵大小。使用Frobenius范数，即$\|A\|_F = \sqrt{\sum_{i,j} A^2_{i,j}}$

## 2.6 特殊类型的矩阵和向量

1. **对角矩阵**：只在主对角线上含有非零元素，其他位置都是零。

   对角矩阵受到关注的部分原因是对角矩阵的乘法计算很高效。

2. **对称矩阵**：对称矩阵的转置和自己相等，即$A=A^{\top}$

3. **正交矩阵**：指行向量和列向量是分别标准正交的方阵，即$A^{\top}A = AA^{\top} = I$。

   如果$x^{\top}y = 0$，那么向量x和向量y**相互正交(orthogonal)**。如果两个向量都有非零范数，那么这两个向量之间的夹角是$90^o$。如果向量相互正交，而且L2范数都为1，那么我们称之为**标准正交**。

## 2.7 特征分解

**特征分解**是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。

方阵A的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量v：
$$
Av = \lambda v
$$
其中，标量$\lambda$称为这个特征向量对应的特征值。

如果v是A的特征向量，那么任何缩放后的向量sv也是A的特征向量。基于此，我们只考虑单位特征向量。

**A的特征分解**可以记作：
$$
A = V \rm{diag}(\lambda) V^{-1}
$$
具体来说，每个**实对称矩阵**都可以分解成实特征向量和实特征值：
$$
A = Q \Lambda Q^{\top}
$$
其中，Q是A的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。

### 矩阵特征分解的性质

1. **特征分解唯一**，当且仅当所有的特征值都是唯一的。
2. 矩阵是**奇异**的，当且仅当含有零特征值。

3. 实对称矩阵的特征分解可以用来**优化二次方程**$f(x)=x^{\top}Ax$，其中限制$\|x\|_2=1$。(当x等于A的某个特征向量时，f将返回对应的特征值。在限制条件下，函数f的最大值是最大特征值，最小值是最小特征值。)
4. 所有特征值都是整数的矩阵称为**正定**
5. 所有特征值都是非负数的矩阵称为**半正定**
6. 所有特征值都是负数的矩阵称为**负定**
7. 所有特征值都是非正数的矩阵成为**半负定**
8. **正定矩阵的性质：**保证$x^{\top} A x$=0=>x=0
9. **半正定矩阵的性质：**保证$\forall x ,x^{\top}Ax \ge 0$

## 2.8 奇异值分解

**奇异值分解**是将矩阵分解为**奇异向量**和**奇异值**。<u>每个实数矩阵都有一个奇异值分解，但不一定有特征分解</u>。

奇异值分解将目标矩阵A分解成三个矩阵的乘积：
$$
A = UDV^{\top}
$$
假设A是一个mxn的矩阵，那么U是一个mxm的矩阵，D是一个mxn的矩阵，V是一个nxn矩阵。

U,V为正交矩阵，D为对角矩阵，但是不一定是方阵。对角矩阵上的元素成为矩阵A的**奇异值**。矩阵U的列向量称为**左奇异向量**，矩阵V的列向量称为**右奇异矩阵**。

### 奇异值分解的性质

1. A的左奇异向量是$AA^{\top}$的特征向量
2. A的右奇异向量是$A^{\top}A$的特征向量
3. A的非零奇异值是$A^{\top}A$特征值的平方根，同时也是$AA^{\top}$特征值的平方根
4. 奇异值分解把矩阵求逆运算推广到了非方阵矩阵上。



## 2.9 Moore-Penrose伪逆

对于非方阵而言，我们希望通过矩阵A的左逆B来求解一下线性方程：
$$
Ax = y \tag{2.44}
$$
等式两边左乘左逆B后，得到：
$$
x = By \tag{2.45}
$$
如果矩阵A的行数大于列数，那么上述方程可能没有解。如果矩阵A的行数小于列数，那么上述矩阵可能有多个解。

矩阵A的伪逆定义为：
$$
A^{+} = \lim_{\alpha \rightarrow 0}(A^{\top}A+\alpha I)^{-1}A^{\top} \tag{2.46}
$$
计算伪逆的实际算法没有基于这个定义，而是使用下面的公式:
$$
A^{+} = VD^{+}U^{\top} \tag{2.47}
$$
其中，矩阵U、D和V是矩阵A奇异值分解后得到的矩阵。对角矩阵D的伪逆$D^{+}$是其非零元素取倒数之后再转置得到的。

> **当矩阵A的列数多于行数时**，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x=A^+y$是方程所有可行解中欧几里得范数$\|x\|_2$最小的一个。
>
> **当矩阵A的行数多于列数时，**可能没有解。在这种情况下，通过伪逆得到的x使得Ax和y的欧几里得距离$\|Ax-y\|_2$最小。



## 2.10 迹运算

迹运算返回的是矩阵对角元素的和:
$$
\rm{Tr}(A) = \sum_{i} A_{i,i} \tag{2.48}
$$
使用迹运算来描述矩阵Frobenius范数:
$$
\|A\|_F = \sqrt{\rm{Tr}(AA^{\top})} \tag{2.50}
$$

### 迹运算的性质

1. Tr(ABC)=Tr(CAB)=Tr(BCA),更一般地，
   $$
   \rm{Tr}(\prod_{i=1}^n F^{(i)}) = \rm{Tr}(F^{(n)} \prod_{i=1}^{n-1}F^{(i)}) \tag{2.52}
   $$
   即使循环置换后矩阵乘积得到的矩阵形状变了，迹运算的结构依然不变。

2. 标量在迹运算后仍然是它自己:a = Tr(a)



## 2.11 行列式

行列式(记作det(A))，是一个将方阵A映射到实数的函数。行列式等于矩阵特征值的乘积。

**行列式的绝对值**可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。如果行列式是0，那么空间至少沿着某一维完全收缩了，**使其失去了所有体积**；如果行列式是1，那么这个转换**保持空间体积不变**。

## 2.12 实例：主成分分析

**主成分分析(PCA)**是一个简单的机器学习算法，可以通过基础的线性代数知识推导。

假设在$\mathbb{R}^n$空间中有m个点${x^{(1)},...,x^{(m)}}$，我们希望对这些点进行**有损压缩**。(有损压缩表示使用更少的内存，但损失一些精度去存储这些点。)

我们希望找到一个**编码函数**,根据输入返回编码，f(x)=c；同时也希望找到一个解码函数，给定编码重构输入，$x \approx g(f(x))$。

**PCA算法**由解码函数而定。具体来讲，为了简化解码器，使用矩阵乘法将编码映射回$R^n$，即$g(c) = Dc$，其中$D \in \mathbb{R}^{n\rm{x}l}$是定义解码的矩阵。

为了降低求解D得难度，我们通常需要假设矩阵D满足以下假设：

1. D中所有列向量都是单位向量
2. D中所有列向量相互正交

**算法求解步骤：**

1. 需要明确如何根据每一个输入x得到一个最优编码$c^*$。**一种方法是最小化原始输入向量x和重构向量$g(c^{*})$之间的距离**。我们使用L2范数来实现：
   $$
   c^* = \arg \min_c \|x-g(c)\|_2 \tag{2.54}
   $$
   使用平方$L^2$范数替代L2范数，因为两者在相同的值c上取得最小值。
   $$
   c^* = \arg \min_c \|x-g(c)\|_2^2 \tag{2.55}
   $$
   该最小化函数可以简化成:
   $$
   \begin{aligned}
   (x-g(c))^{\top}(x-g(c)) &= x^{\top}x-x^{\top}g(c)-g(c)^{\top}x+g(c)^{\top}g(c) \\
   &= x^{\top}x- 2x^{\top}g(c)+g(c)^{\top}g(c)
   \end{aligned}
   $$
   因为第一项不依赖于c，所以我们可以忽略它，得到如下的优化目标:
   $$
   c^* = \arg \min_c - 2x^{\top}g(c)+g(c)^{\top}g(c) \tag{2.50}
   $$
   更进一步，代入$g(c)$的定义：
   $$
   \begin{aligned}
   c^* &= \arg \min_c - 2x^{\top}Dc+c^{\top}D^{\top}Dc \\
   &= \arg \min_c - 2x^{\top}Dc+c^{\top}I_lc \\
   &= \arg \min_c -2x^{\top}Dc+c^{\top}c
   \end{aligned}
   $$
   矩阵D的正交性和单位范数约束:($D^{\top}D=I$)

   我们可以通过向量微积分来求解这个最优化问题:
   $$
   \nabla_c(-2x^{\top}Dc+c^{\top}c) = 0 \\
   -2D^{\top}x + 2c = 0 \\
   c = D^{\top}x
   $$
   由此得到编码函数:
   $$
   f(x) = D^{\top}x \tag{2.66}
   $$
   进一步使用矩阵乘法，我们也可以定义PCA重构操作：
   $$
   r(x) = g(f(x)) = DD^{\top}x \tag{2.67}
   $$
   **接下来，求矩阵D：**

   首先，回顾最小化输入和重构之间$L^2$距离的这个想法。因为用相同的矩阵D对所有点进行编码，我们不能在孤立的看待每个点。

   通过最小化所有维数和所有点上的误差矩阵的Frobenius范数:
   $$
   D^* = \arg \min_{D} \sqrt{\sum_{i,j}(x_j^i-r(x^i)_j)^2}  \ , \rm{subject \ to \  D^{\top}D=I_l} \tag{2.68}
   $$
   令l=1，则D是一个单一向量d。将式(2.67)代入式(2.68)中，简化D为d，问题简化为:
   $$
   d^* = \arg \min_d \sum_i \|x^{(i)}-dd^{\top}x^{i}\|_2^2 \ , \rm{subject \ to \ \|d\|_2=1} \tag{2.69}
   $$
   由于$dd^{\top}$为标量，那么我们可以把上式写成:
   $$
   d^* = \arg \min_d \sum_i \|x^{(i)}-x^{i}dd^{\top}\|_2^2 \ , \rm{subject \ to \ \|d\|_2=1} \tag{2.71}
   $$
   将表示各点的向量堆叠成一个矩阵，记作$X\in \mathbb{R}^{m\rm{x}n}$，其中$X_{i,:} = x^{(i)^{\top}}$，原问题可以重新表述为:
   $$
   d^* = \arg \min_d\|X-Xdd^{\top}\|_F^2 \ , \rm{subject \ to \ d^{\top}d=1} \tag{2.72}
   $$
   暂时不考虑约束，将上式简化为:
   $$
   \arg \min_d \|X-Xdd^{\top}\|_F^2 = \arg \min_d \rm{Tr} ((X-Xdd^{\top})^{\top}(X-Xdd^{\top})) \tag{2.73}
   $$
   最终得到的简化公式为:
   $$
   \arg \max_d \rm{Tr}(d^{\top}X^{\top}Xd) \ , subject \ to \ d^{\top}d=1 \tag{2.84}
   $$
   这个优化问题可以通过特征分解来求解。

   **具体来讲，最优的d是$X^{\top}X$最大特征值对应的特征向量。**

   以上推导**特定于l=1的情况，仅得到第一个主成分。**更一般地，当我们希望得到主成分的基时，矩阵D由前l个最大的特征值对应的特征向量组成。