# 第三章 概率与信息论

概率论是用于**表示不确定性声明**的数学框架。它不仅提供了**量化不确定性**的方法，也提供了用于**导出新的不确定声明**的公理。

概率论主要有两种用途：

1. 首先，概率法则告诉我们AI系统如何推理，据此我们设计一些算法来计算或者估算由概率论导出的表达式。
2. 可用概率和统计从理论上分析我们提出的AI系统的行为。

**概率论**使我们能够提出不确定的声明以及在不确定性存在的情况下进行**推理**。

**信息论**使我们能够量化概率分布中的**不确定性总量**。

## 3.1为什么要使用概率

不确定性有3种可能的来源：

1. **被建模系统内在的随机性**。例如，大多数量子力学的解释，都将亚原子粒子的动力学描述为概率的。
2. **不完全观测**。即使是确定的系统，当我们不能观测到所有驱动系统行为的变量时，该系统也会呈现随机性。
3. **不完全建模**。

## 3.2随机变量

**随机变量**是可以随机地取不同值得变量。

## 3.3 概率分布

**概率分布**用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散的还是连续的。

### 3.3.1 离散型变量和概率质量函数

离散型变量的概率分布可以用**概率质量函数(PMF)**来描述。通常每一个随机变量都会有一个不同的概率质量函数，并且读者必须根据随机变量来推断所使用的PMF。

概率质量函数可以同时作用于多个随机变量。这种多个随机变量的概率分布被称为**联合概率分布**。$P(x=x,y=y)$表示x=x和y=y同时发生的概率。简写为P(x,y).

如果一个函数P是随机变量x的PMF， 必须满足下面的几个条件:

- P的定义域必须是x所有可能状态的集合
- $\forall x \in x, 0 \le P(x) \le 1$。不可能发生的事件，概率为0，一定发生的事件，概率为1.
- $\sum_{x \in X} P(x) = 1$。我们把这条性质称之为**归一化的**。

### 3.3.2 连续型变量和概率密度函数

连续型随机变量的概率分布用**概率密度函数(PDF)**来描述。

如果一个函数p是概率密度函数，必须满足下面几个条件：

- p的定义域必须是x所有可能状态的集合
- $\forall x \in X, p(x) \ge 0$。注意，我们并不要求$p(x) \le 1$
- $\int p(x)dx = 1$

概率密度函数给出了落在面积为$\delta x$的无限小的区域内的概率为$p(x) \delta x$。

## 3.4 边缘概率

假设我们知道了一组变量的联合概率分布，但想要了解其中一个子集的概率分布。这种定义在子集上的概率分布称为**边缘概率分布**。

例如，假设离散型随机变量x和y，并且我们知道P(x,y)。可以根据下面的求和法则来计算P(x):
$$
\forall x \in x, P(x=x) = \sum_{y}P(x=x,y=y) \tag{3.3}
$$
对于连续型变量，我们需要用积分来替代求和：
$$
p(x) = \int p(x,y)dy \tag{3.4}
$$

## 3.5条件概率

某个事件在给定其他事件发生时出现的概率，称之为**条件概率**。在给定x=x时，y=y的发生概率记为$p(y=y|x=x)$。这个条件概率可以通过下面的公式计算:
$$
P(y=y|x=x) = \frac{P(y=y,x=x)}{P(x=x)} \tag{3.5}
$$

## 3.6 条件概率的链式法则

任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相乘的形式:
$$
P(x^{(1)},...,x^{(n)}) = P(x^{(1)})\prod_{i=2}^n P(x^{(i)}|x^{(1)},...,x^{(i-1)}) \tag{3.6}
$$
这个规则被称为概率的**链式法则**或者**乘法法则**。

## 3.7 独立性和条件独立性

如果两个随机变量x和y，满足以下条件，则称这两个变量是**相互独立**的：
$$
\forall x \in x,y \in y,p(x=x,y=y)=p(x=x)p(y=y) \tag{3.7}
$$
如果两个随机变量x和y，在给定随机变量z的条件下相互独立，那么称这两个向量是**条件独立**的：
$$
\forall x \in x,y \in y,z \in z,p(x=x,y=y|z=z)=p(x=x|z=z)p(y=y|z=z)
$$
采用一种简化的表示形式来表示独立性和条件独立性:$x \bot y$表示x和y相互独立，$x\bot y|z$表示x和y在给定z的条件下相互独立。

## 3.8 期望、方差和协方差

### 期望

函数$f(x)$关于某分布P(x)的期望或者期望值是指，当x由P产生，f作用于x时，f(x)的平均值。

对于离散型随机变量，它的期望值通过求和得到:
$$
\mathbb{E}_{x \sim P[f(x)]} = \sum_x P(x)f(x)
$$
对于连续型随机变量，它的期望值通过积分得到：
$$
\mathbb{E}_{x \sim p[f(x)]} = \int p(x)f(x)dx
$$
期望是线性的，例如:
$$
\mathbb{E}_x[\alpha f(x)+\beta g(x)] = \alpha \mathbb{E}_x[f(x)]+\beta \mathbb{E}_x[g(x)]
$$
其中$\alpha$和$\beta$不依赖于x。

### 方差

**方差**衡量的是当我们对x依据它的概率分布进行采样时，随机变量x的函数值会呈现多大的差异：
$$
\rm{Var}(f(x)) = \mathbb{E}[(f(x)-\mathbb{E}[f(x)])^2]
$$
当方差很小时，f(x)的值形成的簇比较接近它们的期望值。方差的平方根被称为**标准差**。

###协方差

**协方差**在某种意义上给出了两个变量**线性相关性的强度**以及这些变量的尺度:
$$
Cov(f(x),g(y)) = \mathbb{E}[(f(x)-\mathbb{E}[f(x)])(g(y)-\mathbb{E}[g(y)])]
$$
协方差的绝对值如果很大，则意味着变量值变化很大，并且它们同时距离各自的均值很远。

> **相关系数**：将每个变量的贡献归一化，为了只衡量变量的相关性而不受各个变量尺度大小的影响。

协方差的性质：

1. 如果两个变量相互独立，那么它们的协方差为0；如果两个变量协方差不为0，它们之间一定没有线性关系。

2. 两个变量的协方差为0，但是两个变量有可能是线性相关的。

随机向量$x \in \mathbb{R}^n$的**协方差矩阵**是一个nxn的矩阵，并且满足:
$$
Cov(x)_{i,j} = Cov(x_i,x_j)
$$
协方差矩阵的对角元是方差:
$$
Cov(x_i,x_i) = Var(x_i)
$$

## 3.9 常用概率分布

### 3.9.1 bernouli分布

伯努利分布是单个二值随机变量的分布。它由单个参数$\phi \in [0,1]$控制，$\phi$给出了随机变量等于1的概率。它具有如下的一些性质。
$$
P(X=1) = \phi \\
P(X=0) = 1- \phi\\
P(X=x)=\phi^x(1-\phi)^{(1-x)} \\
\mathbb{E}_X[X]=\phi \\
Var_X(X) = \phi(1-\phi)
$$

### 3.9.2 Multimoulli分布

**Multinoulli分布**或者**范畴分布**是指在具有k个不同状态的单个离散型随机变量上的分布，其中k是一个有限值。

### 3.9.3 高斯分布

实数上最常用的分布就是**正态分布(normal distribution)**，也称为**高斯分布(Gaussian distribution)**。
$$
\mathcal{N}(x;\mu,\sigma^2)=\sqrt{\frac{1}{2\pi\sigma^2}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)}
$$
正态分布由两个参数控制，$\mu \in \mathbb{R}$和$\sigma \in (0,\infty)$。参数$\mu$给出了中心峰值的坐标，这也是分布的均值:$\mathbb{E}[x]=\mu$。分布的标准差用$\sigma$表示，方差用$\sigma^2$表示。

> 当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两个原因。
>
> - 第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。**中心极限定理**说明很多独立随机变量的和近似服从正态分布。
> - 第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。**我们可以认为正态分布是对模型加入的先验知识最少的分布**。

推广到$\mathbb{R}^n$空间的正态分布被称为**多维正态分布**。它的参数是一个正定对称矩阵$\Sigma$:
$$
\mathcal{N}(x;\mu,\Sigma) = \sqrt{\frac{1}{((2\pi)^n\rm{det}(\Sigma))}}\exp(-\frac{1}{2}(x-\mu)^{\top}\Sigma^{-1}(x-\mu))
$$
参数$\mu$仍然表示分布的均值，只不过现在是向量值。参数$\Sigma$给出了分布的协方差矩阵。

### 3.9.4 指数分布和Laplace分布

在深度学习中，我们经常需要一个在x=0点处取得边界点的分布。为了实现这一目的，我们可以使用指数分布:
$$
p(x;\lambda) = \lambda 1_{x\ge0}\exp(-\lambda x)
$$

> 指数分布用指示函数$1_{x\ge0}$来使得当x取负值时的概率为零。

一个联系紧密的概率分布是**Laplace分布**，它允许我们在任意一点$\mu$处设置概率质量的峰值。
$$
\rm{Laplace}(x;\mu,\gamma) = \frac{1}{2\gamma} \exp(-\frac{|x-\mu|}{\gamma})
$$

### 3.9.5 Dirac分布和经验分布

在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通过**Dirac delta函数**$\delta(x)$定义概率密度函数来实现:
$$
p(x) = \delta(x-\mu)
$$
Dirac delta函数被定义成在除了0以外的所有点的值都为0，但是积分为1.

## 3.10常用函数的有用性质

### logistic sigmoid函数

$$
\sigma(x) = \frac{1}{1+\exp(-x)}
$$

logistic sigmoid函数通常用来产生Bernoulli分布中的参数$\phi$，因为它的范围是(0,1)，处在$\phi$的有效取值范围内。

> sigmoid函数在变量取绝对值非常大的正值或负值时会出现**饱和**现象，意味着函数会变得很平，并且对输入的微笑改变会变得不敏感。

### softplus函数

$$
\zeta(x) = \log(1+\exp(x))
$$

softplus函数可以用来产生正态分布的$\beta$和$\sigma$参数，因为它的范围是$(0,\infty)$。softplus函数名来源于它是另一个函数的平滑(或”软化“)形式，这个函数是：
$$
x^+ = \max(0,x)
$$
一些有用的性质：
$$
\sigma(x) = \frac{\exp(x)}{\exp(x)+\exp(0)} \\
\frac{d}{dx}\sigma(x)=\sigma(x)(1-\sigma(x))\\
1-\sigma(x)=\sigma(-x) \\
\log \sigma(x) = -\zeta(-x) \\
\frac{d}{dx}\zeta(x) = \sigma(x)\\
\forall x \in (0,1),\sigma^{-1}(x) = \log(\frac{x}{1-x}) \\
\forall x>0,\zeta^{-1}(x) = \log(\exp(x)-1) \\
\zeta(x) = \int_{-\infty}^x \sigma(y) dy\\
\zeta(x) - \zeta(-x) = x
$$
函数$\sigma^{-1}(x)$在统计学中被称为分对数(logit)

## 3.11 贝叶斯规则

已知P(y|x)，求P(x|y)。幸运的是，如果还知道P(x)，可以用贝叶斯规则来实现这一目的:
$$
P(x|y) = \frac{P(x)P(y|x)}{P(y)}
$$
$P(y)=\sum_xP(y|x)P(x)$，所以我们不需要事先知道P(y)的信息。

## 3.12 连续型变量的技术细节

连续型随机变量和概率密度函数的深入理解需要用到数学分支**测度论**的相关内容来扩展概率论。

## 3.13 信息论

信息论主要研究的是对一个信号包含信息的多少进行量化。

信息论的基本想法**是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息**。

我们想通过这种基本想法来量化信息。特别是:

- 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。
- 较不可能发生的事件具有更高的信息量
- 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。

为了满足上述3个性质，我们定义一个事件x=x的自信息为:
$$
I(x) = -\log P(x)
$$
以e为底，$I(x)$单位是**奈特(nats)**。一奈特是以$\frac{1}{e}$的概率观测到一个事件时获得的信息量。其他的材料中使用底数为2的对数，单位是**比特**或者**香农**；通过比特度量的信息只是通过奈特度量信息的常数倍。

自信息只处理单个的输出。我们可以用**香农熵**来对整个概率分布中的不确定性总量进行量化:
$$
H(x) = \mathbb{E}_{x\sim P}[I(x)] = - \mathbb{E}_{x\sim P}[\log P(x)]
$$
也记作H(P)。换言之，一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。<u>它给出了对依据概率分布P生成的符号进行编码所需的比特数在平均意义上的下界。</u>

**香农熵的一些性质**：

- 哪些输出几乎可以确定的分布具有较低的熵。
- 哪些接近均匀分布的概率分布具有较高的熵。

- 当x是连续的，香农熵被称为微分熵。

### KL散度

如果对于同一个随机变量x有两个单独的概率分布P(x)和Q(x)，可以使用**KL散度**来衡量这两个分布的差异：
$$
D_{KL}(P\|Q) = \mathbb{E}_{x\sim P}[\log \frac{P(x)}{Q(x)}] = \mathbb{E}_{x\sim P}[\log P(x) - \log Q(x)]
$$
在离散型变量的情况下，KL散度衡量的是，当我们使用一种被设计成能够使得概率分布Q产生的消息的长度最小的编码，发送包含由概率分布P产生的符号的消息时，所需要的额外信息量。

**KL散度的性质：**

- KL散度是非负的；
- KL散度为0，当且仅当P和Q在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是"几乎处处"相同的。因为KL散度是非负的并且衡量的是两个分布之间的差异，它经常被用作分布之间的某种距离。
- 它并不是真的距离，因为它不是对称的：对于某些P和Q，$D_{KL}(P\|Q) \ne D_{KL}(Q\|P)$。

一个和KL散度密切联系的量是交叉熵(cross-entropy)，即$H(P,Q)=H(P)+D_{KL}(P\|Q)$，它和KL散度很像，但是缺少左边一项:
$$
H(P,Q) = -\mathbb{E}_{x\sim P}\log Q(x)
$$
针对Q最小化交叉熵等价于最小化KL散度，因为Q并不参与被省略的那一项。

按照惯例，在信息轮中，我们将这个表达式处理为$\lim_{x\rightarrow0}x\log x = 0$

## 3.14 结构化概率模型

概率图模型，因子分解，马尔科夫随机场，贝叶斯网络。