# 第15章 实际的循环神经网络结构

本章中我们将要展示几个抽象的RNN结构的实例，并给出函数R和O的实际定义。

将要介绍的结构包括简单循环神经网络(Simple RNN,S-RNN),长短期记忆网络(Long Short-Term Memory,LSTM)和门限循环单元(Gated Recurrent Unit,GRU)。

## 15.1 作为RNN的CBOW

选择加法函数作为一个特别简单的R的选择：
$$
s_i = R_{CBOW}(x_i,s_{i-1}) = s_{i-1}+x_i \\ \tag{15.1} 
y_i = O_{CBOW}(s_i) = s_i \\
s_i,y_i \in \mathbb{R}^{d_s},x_i\in \mathbb{R}^{d_s}
$$
式15.1定义了一个连续的词袋模型(CBOW)：来自输入$x_{1:n}$的状态是这些输入之和。

## 15.2 简单RNN

对序列中元素顺序敏感的最简单的RNN形式称为Elman RNN或者简单RNN(Simple RNN,S-RNN)。S-RNN由Elman[1990]提出，并由Mikolov[2012]用于探索的语言模型。

S-RNN有如下的基本形式：
$$
s_i = R_{SRNN}(x_i, s_{i-1}) = g(s_{i-1}W^s+x_iW^x+b)\\ \tag{15.2}
y_i = O_{SRNN}(s_i) = s_i \\
s_i,y_i \in \mathbb{R}^{d_s},x_i \in \mathbb{R}^{d_x},W^x \in \mathbb{R}^{d_x ×d_s},W^s \in \mathbb{R}^{d_s×d_s},b\in \mathbb{R}^{d_s}
$$
式(15.2)的含义是：

- 状态$s_{i-1}$和输入$x_i$分别线性变换，结果相加(连同一个偏置项)，然后通过一个非线性激活函数g(通常是tanh或者ReLU)。

- 位置i的输出与这个位置的隐藏状态相同

式(15.2)等价写法：
$$
s_i = R_{SRNN}(x_i, s_{i-1}) = g([s_{i-1};x_i]W+b)\\ \tag{15.2}
y_i = O_{SRNN}(s_i) = s_i \\
s_i,y_i \in \mathbb{R}^{d_s},x_i \in \mathbb{R}^{d_x},W \in \mathbb{R}^{(d_x+d_s)×d_s},b\in \mathbb{R}^{d_s}
$$
S-RNN与CBOW的主要区别在于非线性的激活函数g。

正是因为加入线性变换后跟随非线性变换的机制使得网络结构对于序列顺序敏感。

> S-RNN在序列标注问题[Xu et al.,2015]以及语言模型[Mikolov et al.,2012]上取得了很好的结果。

## 15.3 门结构

因为梯度消失问题[Pascanu et al.,2012],S-RNN很难有效地训练。**误差信号(梯度)在反向传播过程中到达序列的后面部分时迅速减少，以至于无法到达先前的输入信号的位置，这导致S-RNN难以捕捉到长距离依赖信息。**

**解决方案：**LSTM[Hochreiter and Schmidhuber, 1997]和GRU[Cho et al.,2014b]等基于门的结构被设计出来，用以解决这个问题。

S-RNN的一个明显得问题在于记忆的获取是不受控制的。在每一步的计算过程中，整个记忆状态都被读入，并且整个记忆状态也被改写。

**解决方案：**提供一种更加受控的记忆读取方式

1. **硬性门机制**

   考虑一个二进制的向量$\textbf{g} \in \{0,1\}^n$。考虑一个记忆$\textbf{s} \in \mathbb{R}^d$、输入$x \in \mathbb{R}^d$和门$\textbf{g} \in \{0,1\}^d$，运算$\textbf{s}^{'} \leftarrow \textbf{g} \odot \textbf{x} + (1-\textbf{g})\odot(\textbf{s})$。此运算获取了x中一部分的信息，以及记忆s中的一部分信息，两个部分合并得到新的记忆$s^{’}$。

   **问题：**网络结构中的学习过程需要**函数可微(由于误差反向传播算法)**，而门中使用的二值0-1方式不是可微的。(理论上不使用可微的函数，而使用二值门借助强化学习技术也能学习得到模型。)

2. **可微的门机制**

   令$\textbf{g} \in \mathbb{R}^n$，这个实数值随后通过一个sigmoid函数$\sigma(g)$(为了使得数值限定在(0,1)之间)，并且大多数值都在接近边界的位置(因为大多数值都很接近0,1)。

   当使用门$\sigma(g) \odot x$的时候，x中那些数值接近1的下标被允许通过，而接近0的那些下标则被阻挡。**门的取值可以通过输入和目前的记忆来决定**。并且能够通过使用基于梯度下降的方式来训练一个性能令人满意的网络。

### 15.3.1 长短期记忆网络

长短期记忆网络(LSTM)结构被设计用于解决梯度消失问题。

LSTM结构明确地将状态向量$s_i$分解为两部分，一半称为“记忆单元”，另一半是运行记忆。

**记忆单元**被设计用来保存跨时间的记忆以及梯度信息，同时受控于可微门组件——模拟逻辑门的平滑数学函数。

在每一个输入状态上，**一个门被用来决定由多少新的输入加入记忆单元，以及记忆单元中现有的多少记忆应该被忘记**。

LSTM结构的形式化定义如下：
$$
\begin{aligned}
s_j &= R_{LSTM}(s_{j-1},x_j) = [c_j;h_j] \\ 
c_j &= f \odot c_{j-1} + i \odot z\\
h_j &= o \odot tanh(c_j) \\
i &= \sigma(x_jW^{xi}+h_{j-1}W^{hi})\\
f &= \sigma(x_jW^{xf}+h_{j-1}W^{hf}) \\
o &= \sigma(x_jW^{xo}+h_{j-1}W^{ho}) \\
z &= tanh(x_jW^{xz}+h_{j-1}W^{hz}) \\
y_j &= O_{LSTM}(s_j) = h_j \\
\end{aligned}\\
s_j \in \mathbb{R}^{2.d_h},x_i \in \mathbb{R}^{d_x},c_j,h_j,i,f,o,z \in \mathbb{R}^{d_h},W^{xo} \in \mathbb{R}^{d_x×d_h},W^{ho} \in \mathbb{R}^{d_h×d_h} \tag{15.4}
$$
上式(15.4)的说明：

- 时刻j的状态$s_j$由两个向量组成，分别是$c_j$和$h_j$，$c_j$是记忆组件，$h_j$是隐藏状态组件。

- 三种门结构:$\textbf{i,f,o}$，分别控制输入，遗忘和输出。门的值通过当前输入$x_j$和前一个状态$h_{j-1}$的线性组合通过一个sigmoid激活函数来得到。
- 更新候选项$\textbf{z}$由$x_j$和$h_{j-1}$的线性组合通过一个tanh激活函数来得到。
- 记忆$c_j$被更新：遗忘门控制有多少先前的记忆被保留$f\odot(c_{j-1})$，输入门控制有多少更新被保留$i\odot z$。
- 隐藏状态$h_j$被更新:由记忆$c_j$的内容通过一个tanh激活函数并受输出门的控制来决定。

> 进一步阅读：
>
> - 关于LSTM结构相关的请参考，Alex Graves[2008]的博士论文以及Chris Olah的描述。
> - 使用LSTM构建字符级别的语言模型，参考Karpathy等人[2015]



**实践建议：在训练LSTM网络的时候，Jozefowicz等人[2015]强烈建议将遗忘门的偏置项设置为接近1的值。**

### 15.3.2 门限循环单元GRU

门限循环单元(Gated Recurrent Unit,GRU)也基于门机制，但是总体上使用了更少的门并且没有单独的记忆组件。
$$
\begin{aligned}
s_j &= R_{GRU}(s_{j-1},x_j) = (1-z)\odot x_{j-1} + z \odot \tilde{s_j} \\
z &= \sigma(x_jW^{xz}+s_{j-1}W^{sz})\\
r &= \sigma(x_jW^{xy}+s_{j-1}W^{sr})\\
\tilde{s_j} &= tanh(x_jW^{xs}+(r\odot s_{j-1})W^{sg})\\
y_j &= O_{GRU}(s_j) = s_j \\
\end{aligned}\\
s_j,\tilde{s_j} \in \mathbb{R}^{d_s},x_i \in \mathbb{R}^{d_x},z,r\in \mathbb{R}^{d_s},W^{xo} \in \mathbb{R}^{d_x×d_s},W^{so}\in\mathbb{R}^{d_s×d_s}
$$

- 门(r)用于控制前一个状态$s_{j-1}$的读写并计算一个提出的更新$\tilde{s_j}$。
- 更新的状态$s_j$(同时也作为输出$y_j$)由前一状态$s_{j-1}$和更新$\tilde{s_j}$插值决定，插值过程的比例关系通过门z来控制。

## 15.4其他变体

**非门结构的改进**

1. Mikolov等人【2014】的研究

   **问题定位：**矩阵乘法$s_{i-1}W^s$加上S-RNN更新规则R中的非线性变换g，导致了状态向量$s_i$在每个时间片承受了很大的变化，限制了其记住来自很长时间以前的信息。

   **解决方案：**将状态向量$s_i$拆分成一个缓慢变化的组件$c_i$(”上下文单元“)和一个快速变化的组件$h_i$。

   - 缓慢变化组件根据当前输入和前一时刻的组件的线性插值结果进行更新$c_i=(1-\alpha)x_iW^{x1}+\alpha c_{i-1}(\alpha \in (0,1))$（这种更新方式使得$c_i$能够积累先前的输入）。
   - 快速更新的组件$h_i$通过和S-RNN相似的方式进行更新(不同之处在于将$c_i$也考虑进去了)$h_i = \sigma(x_iW^{x2}+h_{i-1}W^h+c_iW^c)$。
   - 输出$y_i$是状态的缓慢变化部分和快速变化部分的连接:$y_i = [c_i;h_i]$。

   **解释：**限制S-RNN中$c_i$对应的矩阵块$W^s$为多重单位矩阵。

2. Le等人[2015]的研究

   **解决方案：**将S-RNN的激活函数设置为ReLU，将偏置项b初始化为0并把矩阵$W^s$设置为单位矩阵。<u>这导致了一个没有训练的RNN直接复制前一个状态到当前状态，加上当前输入$x_i$的影响，并把负值设为0。</u>

**在可微门结构之外**

1. 可微栈结构(Grefenstette et al.,2015)

   这个栈结构的出栈和入栈操作控制是通过端到端的可微网络来完成的。

2. 神经网络图灵机

   允许读写内容可寻址的记忆。

##15.5 应用到RNN的丢弃机制

在RNN中使用dropout机制的一些观点:

- Pham等【2013】和Zaremba等【2014】提出仅仅在非循环连接的部分使用丢弃。比如深度RNN的层间而不是序列的位置间
- Gal【2015】认为，在对RNN结构进行**变分分析之后**，应该向RNN的所有组成部分(包括循环的和非循环的)应用丢弃机制，**最重要的是在时间步长上保留相同的丢弃掩码**。**<u>也就是说，每个序列采样一次掩码，而不是每个时间步长一次</u>**

