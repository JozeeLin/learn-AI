# 第13章 n元语法探测器：卷积神经网络

简而言之,使用n元语法相对于词袋能获取更多的信息.

一种简单的做法是使用二元词组(bi-gram)词嵌入或三元词组(tri-gram)词嵌入而不是词，然后在词嵌入处理后的n元语法上构建CBOW模型。

**CNN结构** 本章介绍卷积-池化（也叫做卷积神经网络或CNN）结构，专门解决上述建模问题。卷积神经网络被设计用来在大规模结构中**识别出具有指示性的局部预测器**，将它们结合以生成一个固定大小的向量来表示该结构，捕获对当前预测任务信息最多的结构特征。

CNN层的作用是抽取出对于当前整体预测任务有用的有意义的子结构。

## 基础卷积+池化

用于语言任务的卷积核池化背后的主要思想是对一句话的k个词滑动窗口(通常指卷积中的感受野)的每个实例应用一个非线性函数(**滤波器函数**，其相对应于全连接层中的权重)。

滤波器函数将k个词的窗口转换成一个实数值。使用多个这样的滤波器，得到l维向量（每一维对应一个滤波器）捕获窗口内词的重要性。

然后，使用一个”池化“操作，将不同窗口得到的向量通过对l维向量中的每一维取最大值或平均值的方式，结合成一个l为向量。**目的在于聚焦句子中最重要的”特征“，忽略他们的位置。**

**每一个滤波器提取窗口中不同的指示器(子特征)，池化操作则放大了重要的指示器。**

**卷积网络中的反向传播目的：用来调整滤波器函数的参数，使其强化数据中对网络任务更重要的部分。**

### 13.1.1 文本上的一维卷积

考虑一个词序列$w_{1:n} = w_i,..,w_n$每一个对应着他们的$d_{emb}$维的词向量。宽度为k的一维卷积在句子上移动一个大小为k的滑动窗口，对序列中的每个窗口使用同一个”滤波器“，其中的滤波器是一个与权重向量u的内积，其后又通常会使用一个非线性激活函数。

定义操作符$\oplus(w_{i:i+k-1})$为拼接向量$w_i,...,w_{i+k-1}$。第i个窗口的拼接向量即为$x_i=\oplus(w_{i:i+k-1})=[w_i;w_{i+1};...;w_{i+k-1}]，x_i \in \mathbb{R^{k.d_{emb}}}$

然后，将每一个窗口向量应用在滤波器上，得到标量值$p_i$:
$$
p_i = g(x_i.u) \tag{13.1} \\
$$

$$
x_i = \oplus(w_{i:i+k-1}) \tag{13.2} \\
p_i \in \mathbb{R} x_i \in \mathbb{R^{k.d_{emb}}  u \in R^{k.d_{emb}}}
$$



其中，g是一个非线性激活函数。

通常使用l个不同的滤波器$\textbf{u}_1,...,\textbf{u}_l$，将其排列成矩阵U，并增加一个偏置向量$\textbf{b}$。
$$
\textbf{p}_i = g(\textbf{x}_i.\textbf{U}+\textbf{b}) \\ \tag{13.3}
\textbf{p} \in \mathbb{R}^l \ \textbf{x}_i\in \mathbb{R}^{k.d_{emb}} \textbf{U}\in \mathbb{R}^{k.d_{emb}×l}\textbf{b} \in \mathbb{R}^l
$$
每一个向量$\textbf{p}_i$是代表(或总结)了第i个窗口的l个值的集合。 

#### 宽卷积与窄卷积

对于窗口大小为k，长度为n的句子，向量$\textbf{p}_i$的个数有两种计算方式：

1. 窄卷积：n-k+1
2. 宽卷积：在句子的两端填充k个填充词，为n+k+1

#### 两种卷积公式

1. 对序列卷积的描述，n个项(指的是包含n个词的句子)$w_{1:n}$，每一项对应一个d维向量，那么句子向量就由n个d维向量拼接而成1×d.n维向量。

   窗口大小为k且有l个输出值的卷积神经网络基于一个k.d×l的矩阵。

   窗口矩阵在句子向量上对应的k个词窗口进行相乘，每次相乘都会得到l个值。

   l中的每一个值都可以看成是一个k.dx1的向量和句子向量上对应的k个词窗口向量的内积结果。

2. 包含n个词的句子表示成nxd

   卷积操作使用l个不同的kxd矩阵（成为”核”或者“滤波器”）在句子矩阵上滑动，并在核与对应的句子矩阵段上进行矩阵卷积。（两个矩阵的卷积操作定义为对应元素相乘然后累加）

   l个句子卷积核操作，共计产生l个值

以上两种方法本质上是等价的。

#### 信道

对应于计算机视觉上的信道概念，从不同的角度观测数据，每个角度对应一个信道。例如，在文本处理任务中，一个信道对应着词序列，另一个信道对应着词标注序列。

两种角度观测到的数据，通过卷积核操作，最终产生的结果可通过拼接或相加的方式合并成一个输出结果。

#### 总结

卷积层背后的主要思想是对序列中所有的k元语法应用同一个参数化的函数。

提取的特征向量拥有这些特点：

1. 每一个向量代表序列中一个特定k元语法；对k元语法本身以及k元语法内部的词序敏感
2. 对于序列中不同位置的同一个k元语法(同一个滤波器)会得到相同的表示

### 13.1.2 向量池化

假设在文本上进行卷积得到了m个向量$\textbf{p}_{1:m} \, \textbf{p}_i\in\mathbb{R}^l$，然后，使用**池化层**变成一个向量$\textbf{c} \in \mathbb{R}^l$表示整个序列。

<u>理想情况下，向量c捕获了序列重要信息的实质内容。而需要编码进向量c的句子的重要信息的实质内容是任务相关的。</u>

在训练过程中，向量c被送给网络中后续的层中（例如MLP），最终到达用于预测的输出层。(后续网络的输入可以是单独的向量c，也可以是向量c与其他输入的结合)。

> 与词嵌入思想类似，我们可以使用卷积池化结构把一个句子编码成一个固定长度的向量，此向量有着相同预测性信息的句子彼此接近。

常见的池化操作：

1. max-pooling
2. average-pooling
3. k-max pooling
4. dynamic pooling



### 13.1.3变体

还可以平行使用多个卷积层。例如，我们可以设计4个不同的卷积层，每个窗口大小为2-5不等，捕获序列中不同长度的k元语法。每一个卷积层的结果将被池化，然后将得到的向量拼接后送入接下来的处理中。

## 13.2 其他选择：特征哈希

用于文本的卷积神经网络是非常高效的连续k元语法特征探测器。**缺点是：它们所需要的大量矩阵乘法会导致不可忽视的计算开销。**

**解决方案：**一个时间效率更高的选择是直接使用k元语法的词嵌入，然后使用average-pooling或max-pooling对k元语法进行池化(得到连续词袋n元语法表示)

**缺点：**（维度太高）需要为每一个可能的k元语法分配一个专门的词嵌入，由于训练数据集中的k元语法数量可能非常大，该方法会占用过高的存储空间。

**解决方案：**使用特征哈希。特征哈希的背后思想是，我们不需要预先计算词汇-下标的映射。

## 13.3 层次化卷积

一维卷积方法可以看作一个n元语法探测器。一个窗口大小为k的卷积层学习识别输入中具有指示性的k元语法。

这种方法可以扩展成层次化卷积层，卷积序列逐层相连。

### 步长，膨胀和池化

### 参数捆绑和跨层连接

