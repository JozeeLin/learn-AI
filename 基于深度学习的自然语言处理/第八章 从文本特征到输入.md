# 第八章 从文本特征到输入

本章将讨论如何将一系列核心特征转换成分类器可接收的特征向量的细节.

## 编码分类特征

处理自然语言时用到的大部分特征是离散,分类特征,比如单词,字母和词性.**如何将这样的分类数据编码成便于统计分类器使用的形式呢?**我们将讨论**独热编码(onehot)**和**稠密嵌入向量**两种方案,以及两种方案间的权衡和关系.

### 独热编码

就是创建一个大小为N的词典,然后对词典中的每个词用N维向量来进行表示,该向量由0和1组成;假设被表示的词在词典中的编号为i,在N维向量中,在第i维的元素为1,其他维的元素为0,以此来表示该词.

### 稠密编码(特征嵌入)

从稀疏输入的线性模型到深度非线性模型的最大概念跨越可能就是不再以独热中的一维来表示各个特征,转而使用稠密向量表示;<u>也就是每个核心特征都被嵌入到d维空间中,并用空间中的一个向量表示.(不同特征类型可能被嵌入到不同的空间中,比如用100维空间嵌入词特征,用20维嵌入词性特征,然后把两个特征向量拼接起来)</u>

### 稠密向量与独热表示

- 独热表示 每个特征有单独一维
  - 独热向量维度与不同特征的数目相同
  - 特征间完全互相独立,特征"word is dog"与"word is thinking"和"word is cat"同样不想似
- 稠密表示 每个特征为一个d维向量
  - 向量维度是d
  - 模型训练会导致相似特征对应相似向量,相似特征间的信息是共享的

优缺点对比:

- 稠密低维向量是可计算:因为大部分神经网络工具包不能很好地处理高维稀疏向量,然而这只是一个技术障碍,可以通过工程方法解决

- 稠密表示的主要好处是**很强的泛化能力**:如果我们假设某些特征可能提供相似的线索,那么提供一种能够捕捉这些相似性的表示方法是值得的.

  <u>这种好的词向量(也被称为预训练嵌入)能够通过基于分布假设的算法在大规模文本语料上得到.</u>

- 如果同一组的不同特征间有相互关系(如词性特征),那么让网络弄清相互关系并且通过**共享参数**获得一些统计强度是值得的.

- 在特征空间相对较小并且训练数据比较充足或者我们**不希望共享不同词间的统计信息**时,使用one-hot表示会取得结果上的提升.

> Chen和Manning[2014],Collobert和Weston[2008],Collobert等人[2011]倡导使用稠密可训练的嵌入向量来表示所有特征.
>
> Johnson和Zhang[2015]使用稀疏向量编码的神经网络结构可以.



## 组合稠密向量

每个特征对应一个稠密向量,需要用某种方式将不同的向量组合起来,主要有拼接,相加(或者平均)和同时使用拼接与相加.

### 基于窗口的特征

考虑以位置i为中心词,两边各包含k个单词的窗口,设窗口大小为k=2,那么需要编码位置在i-2,i-1,i+1,i+2上的词,窗口内的词为a,b,c和d,对应的词向量为**a,b,c,d**.

- 不关心窗口内词的相对位置,那么通过求和的方式对窗口中的词向量进行组合
- 关心相对位置,使用拼接的方式**[a;b;c;d]**,尽管同一个词不论在什么位置都用相同的向量表示,但词的位置信息可以通过拼接位置反应出来.
- 不太关心词的顺序,但相对于离中心较远的词,更加注重距中心较近的词,那么使用加权求和的方式进行组合**[$\frac{1}{2}$a+b+c+$\frac{1}{2}$d]**.
- 如果我们关心特征是出现在中心词前还是中心词后,但不关心窗口内的词距中心词的距离,那么可以采用拼接和相加组合的编码方式,**[(a+b);(c+d)]**

> **注意:**当描述以拼接向量x,y和z作为输入的网络层时,有些作者使用显示拼接([x;y,z]W+b),有些使用仿射变换(xU+yV+zW+b),**如果仿射变换中的参数矩阵U,V,W互不相同(矩阵通常是不同的,但是也可以矩阵共享),那么两种描述方式是等价的**

### 可变特征数目:连续词袋

前馈神经网络使用固定维度的输入,这样能够很容易地与抽取固定数目特征的抽取函数相适应.通过拼接组合向量时,如果输入的特征不固定(如在文本分类任务中,句子中的词特征不固定),那么就无法得到固定维度的输入了.

**解决方案:**

**连续词袋(CBOW)**方法[Mikolov et al., 2013b].CBOW和传统的不考虑顺序信息的词袋表示非常相似,通过相加或者平均的方式组合特征的嵌入向量.
$$
CBOW(f_1,...,f_k) = \frac{1}{k}\sum_{i=1}^{k}v(f_i) 
$$
**加权CBOW(WCBOW)**是CBOW的一种简单变换,为不同的向量赋予不同的权重:
$$
\rm{WCBOW}(f_1,...,f_k) = \frac{1}{\sum_{i=1}^k a_i}\sum_{i=1}^k a_iv(f_i)
$$
每个特征$f_i$都有对应的权重$a_i$,表明特征的相对重要性.比如在文本分类任务中,特征$f_i$可能对应文本中的一个词,相关的权重$a_i$可以是这个词的tf-idf值.

## 独热和稠密向量间的关系

实际上,在训练神经网络时,使用稀疏独热向量作为输入,意味着使网络的第一层从训练数据中学习特征的稠密嵌入向量.

### 独热特征与稠密向量之间的映射

每个分类特征值$f_i$(独热向量)被映射为稠密的d维向量(稠密向量)$v(f_i)$,通过*嵌入层*或*查找层(还记得Tensorflow里的lookup函数吗?)*来进行映射.

对于包含|V|个词的词表,每个词用d维向量嵌入,所有词的向量的集合可以看作一个|V|xd的嵌入矩阵**E**,矩阵的每一行对应一个嵌入的特征.

**$f_i$**为|V|维独热表示向量,除一维对应第i个特征的值为1,其余维均为0,乘法操作$f_iE$会选择$E$的对应行,因此$v(f_i)$通过**E**和$f_i$来定义:
$$
v(f_i) = f_iE
$$
相似地,
$$
\rm{CBOW}(f_1,...,f_k) = \sum_{i=1}^k(f_iE) = (\sum_{i=1}^{k}f_i)E
$$
尽管以上的数学公式非常优雅,但是矩阵乘法不够高效;我们可以使用等效的基于哈希的数据结构来将特征映射到它们对应的嵌入向量,不需要通过独热表示.

### 独热特征和稠密特征之间的微妙区别

考虑一个接受传统稀疏表示作为输入向量的网络,而且**不包括嵌入层**,假设可获得的特征集合为V,我们有k个特征$f_1,...,f_k \in V$, 网络的输入为:
$$
x = \sum_{i=1}^k f_i , x \in N_+^{|V|} 
$$
网络的第一层(忽略非线性激活函数)为:
$$
xW+b = (\sum_{i=1}^k f_i) W + b\\
W \in R^{|V|\rm{x} d}, b\in R^d
$$
该层从W选择输入x中的特征对应的行求和,然后加上一个偏置项.

这层网络和CBOW表示的嵌入层非常相似,矩阵W作为嵌入矩阵:

- **第一个区别**在于该层引入偏置项,并且嵌入层一般不经过非线性激活函数.
- **第二个区别**在于每个特征必须接受一个单独的向量(W中的行),而嵌入层中则允许如"dog"这个词,在任何位置,任何上下文下,它的稠密向量都是一样的,共享的.

> **注意**:当遇到多层前馈神经网络时,稠密和稀疏输入之间的区别会比起初看起来地更小.

## 杂项

### 距离与位置特征

一个句子中**两个词的线性距离**可能作为一个提供信息的特征.

在事件抽取任务中,给定一个触发词的一个候选元素词,需要预测元素词是否为该触发词所代表的事件的一个元素.

> **事件抽取任务**从预先定义好的事件类别中识别事件,如识别"购买"事件或"恐怖袭击"事件.每个事件类型可以被不同事件触发词(通常是动词)触发,并且有若干个槽(元素)需要填充(如谁进行购买?购买什么物品?买了多少?).

在共指消解任务中(如果前面有提及的实体,判断代词he或者she指代的具体内容),可能会给定一组代词和候选词,预测它们是否为共指关系.

<u>触发词和元素之间的距离(或相对位置)对于这类预测任务是一个非常强的信号.</u>

> 距离特征采用与其他特征类型相似的方式编码:每一组关联到一个d维向量,这些距离嵌入向量作为网络参数进行训练.[dos Santos et al. , 2015, Nguyen and Grishman, 2015, Zeng et al. 2014, Zhu et al. , 2015a].

### 补齐,未登录词和词丢弃

- **补齐**

  有时特征抽取器会寻找一些不存在的东西,比如从句法树中抽取特征时,可能需要找到一个词的最左依存结点,但**可能有的词左侧没有任何依存节点**.

  <u>这种情况下该怎么处理呢?</u>

  - 当使用词袋特征方法(如相加)时,可以直接忽略
  - 使用拼接方法时,可以用零向量进行填充.

  这两种方法对于一些问题来说可能只是次优解.**推荐做法是添加一个特殊符号(补齐符号padding)到嵌入词表中,并且在上述情况中使用相应的补齐向量.**

- **未登录词**

  另一种特征向量缺失的情况是**词表外的条目**,提取到的词不是训练词表中的一部分,所以没有对应的嵌入向量.**这与补齐不同,因为条目是存在的,但是却无法进行映射到对应的嵌入向量**.

  <u>怎么处理这种情况?</u>

  解决方案是保留一个特殊符号UNK表示未知记号来应对这种情况.

  不建议共享补齐和未登录向量,因为它们代表两种不同的情况

- **词签名**

  **处理未登录词**的另一种技术是将词的形式回退到词签名.使用UNK符号表示未登录词是**将所有未登录词回退到同样的签名**.

  <u>怎么做?</u>

  根据要解决的问题的不同,可能用更加细粒度的策略,比如用'-ing'符号代替以ing结尾的未登录词,以'-ed'符号代替以ed结尾的未登录词.

  映射列表是手工构建的,以反应回退模式.**这种方法实际中很常用,但很少深度学习论文中阐明**.

  另外,也可让模型自动学习回退行为的方法,不需要人工定义回退模式(参见10.5.5关于子词单元讨论的部分),这种方式并不高效,因为**硬编码模式同样有效而且计算效率更高**.

- **词丢弃**

  为未登录词保留一个特殊的嵌入向量并不够.

  原因在于:**未登录词只会在预测阶段出现,而训练阶段则不会出现,也就是说在训练阶段模型并没有学习如何处理遇到未登录词的情况,那么未登录词向量便不会被更新,这与在预测阶段使用一个随机向量来表示未登录词等效**.

  <u>怎么处理这种情况?</u>

  模型需要在训练时学习处理未登录词.怎么做?

  - 用未登录符号替换训练集中所有或部分低频特征(比如在预处理时用'unknown'替换词频低于某个阈值的词).但是这种方案的缺点是会丢失一些训练数据,被替换的低频词不会接收任何反馈信号.

    **也就是说,未登录词就是视为低频词**

  - 另一种方式为**词丢弃**:在训练集中抽取特征时,**用未登录符号随机替换单词**.

    这种替换应**基于词频**:低频词相较于高频词更可能被替换.

    **随机替换应在运行时决定**,曾经被丢弃的词在下次出现时(比如在训练数据的不同迭代间)不一定会再次被替换.

    目前没有确定的公式来决定丢弃的比率.

- **使用词丢弃进行正则化**

  除了更好的适应未登录词之外,**词丢弃可能会有助于避免过拟合和通过让模型不过分依赖任何当前词来提高鲁棒性[Iyyer et al.,2015]**

  > 事实上,Iyyer等[2015]建议使用概率为p的伯努利试验来进行词丢弃,而不考虑词频.

  **注意**:使用词丢弃进行正则化方法时,在某些情况下你可能不希望用未登录符号来代替丢弃的词.

### 特征组合

传统的使用核方法或具体的多项式核来进行特征组合的选择,虽然其函数是凸函数,确保了优化问题有最优解,但是其算法复杂度是随着训练数据规模线性增加的,训练速度太慢,不适合大规模数据上进行训练.每个样本的处理时间与数据规模相关.

而神经网络模型的分类计算复杂度随网络规模线性增加,与训练数据大小无关,当然在训练的过程中,如果需要进行多次的迭代训练,训练时间也是与数据规模呈线性关系的.但每个样本的处理时间只与网络的复杂度有关.

### 向量共享

<u>什么时候使用共享嵌入向量词表,什么时候使用不同的嵌入向量词表?(**通常基于经验来处理这种问题**)</u>

以词性标注来举例,词性标注的两个特征分别是目标词前一个词,目标词后一个词,分别获取这两个特征的嵌入向量,然后拼接起来作为整体的特征向量.

- 假设目标词前一个词和目标词后一个词都是dog,那么如果dog这个词出现在目标词前的意义与出现在目标词后的意义不一样,那么就是用不同的嵌入向量来分别表示dog.
- 如果dog这个不管出现在目标词前还是后,都是一样的含义,那么就是用共享嵌入向量来表示.

### 维度

<u>应该为每个特征分配多少维?</u>

在目前的研究中,词嵌入向量的维度可以在50到几百之间,在某些极端的情况下可能达到上千维.

既然向量的维度直接影响内存需求和处理时间,**经验方法是对几个不同大小的维度进行实验,然后在速度和任务准确性之间取得一个良好的平衡.**

### 嵌入的词表

基于训练集或者使用用于训练预训练嵌入的训练集的词表.同时建议在词表中加入padding和unk符号.

### 网络的输出

对于具有k个类别的多分类问题,网络的输出是一个k维向量,每一维表示一个输出类别的强度.

> **历史注记:**Bengio等[2003]关于神经语言模型的工作使通过稠密向量表示词并作为神经网络输入的方法变得流行起来,这种方法由Collobert,Weston及同事[Collobert and Weston, 2008, Collobert et al., 2011]的开创性工作引入NLP任务中.使用嵌入进行表示不局限于词,Chen和Manning[2014]使得这种方法应用于任意特征变得流行.

## 例子:词性标注(使用成分树)

> 结合7.4节来看

词性标注需要涉及三个布尔问题:

1. 单词是否为大写
2. 单词是否包含连字符
3. 单词是否包含数字
4. 单词的词嵌入向量
5. 目标词的前缀
6. 目标词的后缀

以上6个特征拼接成完整的特征作为词的新嵌入向量$v$.

已预测出的词性特征:

1. 目标词前第二个词的词性向量$p_{i-2}$
2. 目标词前第一个词的词性向量$p_{i-1}$

输入向量则表示为:

- 窗口对应的向量表示为$[v_{i-2},v_{i-1},v_i,v_{i+1},v_{i+2},p_{i-1},p_{i-2}]$,此处窗口k=2

> 1.对词性标注来说,大写是一个非常重要的线索
>
> 2.对于词的前缀后缀信息的冗余问题,可以参考16.2.1节中的解决方案,该方案使用字符级循环神经网络(RNN)来获取前缀,后缀和各种其他词形的特性.



## 例子:弧分解分析

> 结合7.7节来看

<u>如何对7.7节给出的一系列特征编码成一个向量(作为网络的输入向量)的形式?</u>

首先,定义一个特征函数$x=\phi(h,m,sent)$来获取一个句子的向量表示,参数为单词,词性,头词(h)和修饰词(m)的位置等组成.

接下来,逐个对这些参数详细来分解:

1. 头词,它的词性以及头词周围大小为5的窗口中的单词和词性

   $v_w(w_i)$表示$w_i$的嵌入词向量,$v_t(p_i)$表示$w_i$词的词性向量
   $$
   v_i = [v_w(w_i);v_t(p_i)] 
   $$
   h表示头词,m表示修饰词
   $$
   h = [v_{h-2};v_{h-1};v_h;v_{h+1};v_{h+2}] \\
   m = [v_{m-2};v_{m-1};v_m;v_{m+1};v_{m+2}]
   $$
   