# 第14章 循环神经网络：序列和栈建模

处理语言数据的时候，一种普遍的形式是处理序列。我们可以看到前馈网络通过向量拼接和向量相加的方式可以兼容任意的特征函数(CBOW)。CBOW可以把一个序列表示成特定长度的向量，但是表示存在局限性并且忽略了特征的**序关系**。

卷积神经网络同样可以把一个序列表示成特定长度的向量，同时对词序敏感，因此效果优于CBOW。**但是这种敏感程度大多仅局限于局部模式内，并没有考虑到模式间的顺序，从而使得这种表示与真正的序列存在较大差距。**

循环神经网络可以将任意长度的序列表示成定长的向量，同时关注输入的**结构化属性**。尤其是那些带有门结构如LSTM和GRU的各种RNN结构(**在捕获线性输入的统计规律方面非常有效。**)。

本章将RNN描述成一种抽象形式：一个将序列输入翻译成定长向量的接口，其得到的定长向量可进一步接入到更大规模的网络中。

在第9章中，我们讨论了语言模型和马尔科夫假设。**RNN允许语言模型不依赖于马尔科夫假设，并将完整的句子历史（前面的所有单词）作为下一词的条件**。<u>基于此特性，语言模型可以作为一个生成器。</u>

## 14.1RNN抽象描述

具体来说，RNN是依靠一个接收状态向量$s_{i-1}$作为输入返回新的状态向量$s_i$的函数R来**递归定义**的。然后使用一个简单的确定性函数$O(.)$将状态向量$s_i$映射成输出向量$y_i$。

构建一个RNN很大程度上类似于构建一个前馈网络，必须指定输入$x_i$和输出$y_i$的维度。状态$s_i$的维度是输出维度的一个函数：
$$
\rm{RNN^*(x_{i:n};s_0) = y_{1:n}} \\
\rm{y_i = O(s_i)} \\
\rm{s_i = R(s_{i-1},x_i)}\\
\rm{x_i \in R^{d_{in}},y_i \in R^{d_{out}},s_i\in R^{f(d_{out})}}
$$
其中，函数R和O对于序列的所有位置都是相同的。而状态向量$s_i$则被保持并通过调用R来传递，以此使得RNN实现对状态计算过程的记录。**RNN网络的训练过程就是设置R和O的参数使得状态可以为我们尝试解决的任务表达出有效的信息。**

## 14.2RNN的训练

为了训练一个RNN网络，所需要做的即为对给定的输入序列构建一个展开的计算图，**为展开的图添加一个损失节点**，然后使用反向（反向传播）算法计算关于该损失的梯度。**RNN的反向传播过程被称为沿时间展开的反向传播（BPTT）**。

**问题：如何添加损失节点？**

## 14.3RNN常见使用模式

### 14.3.1 接收器

将监督信号仅置于最后的输出向量$y_n$上。从这个角度看，RNN是作为一个接收器来进行训练的。我们观测最后一个状态，然后决策一个输出。这样的RNN模式，它的损失以函数$y_n = O(s_n)$的形式定义。<u>通常RNN的输出向量$y_n$会被送到一个全连接层或MLP中从而产生一个预测。然后误差梯度通过序列的其余部分反向传播。（**这种监督信号可能由于梯度消失的问题而难以训练长序列**）</u>。

### 14.3.2编码器

编码器的监督信号仅使用了最后的输出向量$y_n$。然而，不同于接收器的仅基于最后向量进行预测，此处的最终向量被当做序列信息的一个编码，和其他的信号共同作为附加信息来使用。

### 14.3.3传感器

将RNN作为传感器，对于每一个读取的输入产生一个输出$\hat{t_i}$，然后，基于真实标签$t_i$为每一个输出$\hat{t_i}$计算一个局部损失信号$L_{local}(\hat{t_i},t_i)$，对于展开序列损失即为$L(\hat{t}_{1:n},t_{1:n})=\sum_{i=1}^nL_{local}(\hat{t}_i,t_i)$，或不使用加和而使用其他的诸如平均或加权平均的形式。

> 使用RNN作为传感器使我们可以放宽传统语言模型和HMM标签器中的马尔科夫假设，给定整个预测历史作为条件输入。
>
> RNN传感器的一个特殊的情况是RNN生成器以及相关的条件生成(也被称作编码器-解码器)和带注意力机制的条件生成结构。这些将在第17章中讨论。

## 14.4 双向RNN

类似于RNN放宽了马尔科夫假设，允许向后回顾任意长度的历史，biRNN则放宽了固定窗口大小的假设，允许在序列内部向前或者向后看任意远的距离。

考虑输入序列$x_{1:n}$。对于每一个输入位置，biRNN获取两个独立状态$s_i^f$和$s_i^b$。前向状态$s_i^f$基于$x_1,x_2,...,x_i$，后向状态$s_i^b$基于$x_n,..x_{n-1},...x_i$。前向后向状态由不同的RNN生成。第一个$RNN(R^f,O^f)$以序列$x_{1:n}$作为输入，第二个$RNN(R^b,O^b)$以序列$x_{n:1}$作为输入。然后使用前向后向状态组合成状态表示$s_i$。

第i个位置的输出基于两个输出向量的拼接$y_i=[y_i^f;y_i^b]=[O^f(s_i^f);O^b(s_i^b)]$，换句话说，biRNN对序列中第i个单词的编码$y_i$是两个RNN的连接，一个RNN从序列起点开始读取，一个RNN从序列的终点开始读取。

$\rm{biRNN(x_{i:n},i)}$定义为第i个位置的输出向量:
$$
\rm{biRNN}{x_{1:n},i} = y_i = [RNN^f(x_{1:i});RNN^b(x_{n:i})]
$$
向量$y_i$直接用来进行预测(通常会送入MLP网络中，再进行预测)，或作为更为复杂网络的输入的一部分。(<u>第i个位置的误差的梯度会同时传播给前向和后向两个RNN。</u>)

> NLP中用于序列标注的biRNN的使用，是由Irsoy和Cardie[2014]提出的。



## 14.5堆叠RNN

考虑k个RNN，$\rm{RNN_1,RNN_2,...RNN_k}$，第j个RNN拥有状态$s_{1:n}^f$和输出$y_{1:n}^j$。第一个RNN的输入是$x_{1:n}$，第j个RNN(j>=2)的输入是其下方RNN的输出$y_{1:n}^{j-1}$。整体结构的输出是最后一个RNN的输出$y_{1:n}^k$。这种层次化结构通常称为deep RNN。通常情况下，deep RNN的确比浅层的表现更好。

## 14.6 用于表示栈的RNN

从直观上来看，栈本质上是一个序列，因此栈的状态可以通过将栈内元素送入RNN后得到的最终对整个栈的编码来表示。

> 这种建模方法有Dyer等人【2015】以及Watanabe和Sumita【2015】各自独立提出，用于基于转移的依存分析。



