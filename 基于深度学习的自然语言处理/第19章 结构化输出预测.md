# 第19章 结构化输出预测

在自然语言处理中的许多任务涉及结构化输出，在这种任务中我们期望得到的输出不是一个类标签或者类标签的分布，<u>而是一个如序列、树或者图的结构化对象。</u>

典型的任务包括序列标注(例如词性标注)、序列分割(例如组块分析和命名实体识别)、句法分析以及机器翻译。

本章中，我们将讨论神经网络模型在结构化任务中的应用。

## 19.1 基于搜索的结构化预测

结构化预测一般采用基于搜索的方法。

基于搜索的结构化预测可以被形式化为在可能的结构中进行搜索的问题：
$$
\mathrm{predict(x) = argmax_{y\in y(x)} score_{global}(x,y)} \tag{19.1}
$$
其中，x是输入结构，y是对应的输出(<u>在一般情况下x是一个句子，而y是该句子上的标签赋值或者句法树。</u>)，$y(x)$是在x上所有有效输出结构的集合。

我们希望**找到一个输出y，使得x，y组合的分值最大**。

### 19.1.1 基于线性模型的结构化预测

评分函数建模使用的是线性函数:
$$
\mathrm{score_{global}(x,y) = w.\Phi(x,y)} \tag{19.2}
$$
其中$\Phi$是一个特征抽取函数，w是一个权重向量。

**为了是对于最优输出y的搜索成为可行**，将结构y分解成小的部分(子结构)，并根据这些子结构来定义特征函数。如下式所示，$\phi(p)$是对于子结构进行特征抽取的函数：
$$
\mathrm{\Phi(x,y) = \sum_{p\in parts(x,y)} \phi(p)} \tag{19.3}
$$
每个子结构的分值单独进行计算，整个结构的分值则为所有子结构分值之和:
$$
\begin{aligned}
\mathrm{score_{global}(x,y)} &= w.\Phi(x,y)\\
&=\mathrm{w.\sum_{p\in parts(x,y)} \phi(p)}\\
&= \sum_{p\in y} w.\phi(p) \\
&= \mathrm{\sum_{p \in y} score_{global}(p)}
\end{aligned}
\tag{19.4}
$$
<u>之所以对y进行分解，是为了找到一种推断算法，使得在给定每个部分的分值情况下，能够高效地搜索分值最高的结构。</u>

### 19.1.2 非线性结构化预测

现在，将子结构上的线性评分函数用神经网络来代替:
$$
\mathrm{score_{global}(x,y) = \sum_{p\in y}score_{global}(p) = \sum_{p\in y}NN(\phi(p))} \tag{19.5}
$$
其中$\phi(p)$将子结构p映射为$d_{in}$维向量。

**结构化预测中的常用目标是使得正确结构y的分值比其他所有错误结构$y^{'}$的分值更高**，从而有如下损失函数(广义感知机[collins 2002]):
$$
\mathrm{\max_{y^{'}} score_{global}(x,y^{'})-score_{global}(x,y)} \tag{19.6}
$$
该最大化过程通常使用基于动态规划或类似的搜索算法来实现。

在具体实现上，首先，为每个可能的子结构建立一个计算图$CG_p$，并计算它的分值。然后，根据每个子结构的分值进行推断(即搜索)以找到分值最高的结构$y^{'}$。接着，将计算图中正确(预测)结构y($y^{'}$)中所包含的各个子结构的输出节点相加，从而得到表示总分值的结点$CG_y(CG^{'}_y)$。下一步，使用一个“减法”结点连接$CG_y$和$CG^{'}_y$，得到$CG_l$。最后计算梯度。

**广义感知机没有考虑正负样例之间的间隔(margin)**，基于间隔的hinge损失则更适合:
$$
\mathrm{max(0,m+max_{y^{'}\ne y} score_{global}(x,y^{'})-score_{global}(x,y))} \tag{19.8}
$$


**代价增强训练**

在大多数情况下，损失函数、正则化以及Smith[2011]中所介绍的方法都能够很容易地迁移到神经网络框架中，尽管丢失了模型的凸性以及与之相关的很多理论保证。

**问题：**式(19.6)和式(19.8)在最大化过程中极有可能出现梯度消失的情况。

**解决方案:**代价增强训练方法的主要思想是修改最大化过程，使我们在寻找结构$y^{'}$时，不仅要求它在当前模型下分值较高，还要使得它有相对多的错误成分(子结构)。形式上，hinge目标函数变为:
$$
\mathrm{max(0,m+max_{y^{'}\ne y}(score_{global}(x,y^{'})+\alpha \Delta(y,y^{'}))-score_{global}(x,y))} \tag{19.9}
$$
其中$\alpha$是用来表示$\Delta$与模型分值之间相对重要性的超参数(标量)，$\Delta(y,y^{'})$则为计算$y^{'}$与y之间不同子结构数目的函数:
$$
\Delta(y,y^{'}) = |\{p:p\in y^{'},p\notin y\}| \tag{19.10}
$$
在实际的训练过程中，通常通过增加$\alpha$来实现这个目标的有效性。

代价增强推断倾向于找出错误较多的结构，以保留更多不会被抵消的损失项，进而带来更有效的梯度更新。

### 19.1.3 概率目标函数(CRF)

基于错误的和基于间隔的损失函数都在试图使正确结构的分值高于错误结构，**但不关心在分值最高的结构之外的其他结构之间的分值排序或者分值之间的差距**。

判别式概率损失则为给定输入下的每个可能的结构计算一个概率，并最大化正确结构的概率。概率损失考虑了所有可能结构的分值，而不仅仅是分值最高的结构。

在概率框架(也叫做条件随机场，CRF)下，每一个子结构的得分被看做是一个团势函数，则每个结构y的分值可定义为:
$$
\begin{aligned}
\mathrm{score_{CRF}(x,y) = P(y|x) } &= \mathrm{\frac{e^{score_{global}(x,y)}}{\sum_{y^{'}\in y(x)} e^{score_{global}(x,y^{'})}}} \\
&= \mathrm{\frac{exp(\sum_{p\in y}score_{lobal}(p))}{\sum_{y^{'}\in y(x)} exp(\sum_{p\in y^{'}}score_{local}(p))}} \\
&= \mathrm{\frac{exp(\sum_{p\in y}NN(\phi(p)))}{\sum_{y^{'}\in y(x)} exp(\sum_{p\in y^{'}}NN(\phi(p)))}}
\end{aligned}
\tag{19.11}
$$
该评分函数定义了一个条件概率分布$P(y|x)$。

对于一个给定的训练样本(x,y)，其损失则为:
$$
\mathrm{L_{CRF}(y^{'},y) = - \log score_{CRF}(x,y)} \tag{19.12}
$$
CRF损失可以看成是(硬)分类(hard-classification)问题的交叉熵损失在结构化预测问题上的扩展。

### 19.1.4 近似搜索

对于某个预测问题并不存在高效的搜索算法。

在这种情况下，采用近似推断算法，如柱搜索(beam search)。

### 19.1.5 重排序

在重排序框架中，首先，使用一个基础模块获得分值最高的k个候选结构(k-best)。然后，再训练一个更加复杂的模型为这k个候选结构重新评分，使得其中正确的结构分值最高。

### 19.1.6 参考阅读

（略）

## 19.2 贪心结构化预测

贪心方法将结构化预测问题分解为一系列的局部预测问题，并训练一个分类器来完成好每一个局部的决策。

在测试阶段，以贪心的方式使用该分类器。这种方法的典型实例，有自左向右的序列标注模型[Gimenez and Marquez,2004]以及基于转移的贪心句法分析[Nivre,2008]。

根据定义，贪心方法是启发式的，它可能会受到错误传播(error-propagation)的影响，即决策序列早期的预测错误无法得到修正，而且会向后传递从而导致更大的错误。

基于biRNN特征提取器训练的贪心局部模型成为了贪心全局模型:每个决策都将参考整个句子的信息，使得决策过程不太容易因意料之外的输出而感到"惊讶"。

在句法分析上的研究表明，基于全局biRNN特征提取器训练的贪心预测模型与结合全局搜索和局部特征提取器的基于搜索的模型准确性相当[Cross and Huang,2016a, Dyer et al.,2015]

## 19.3 条件生成与结构化输出预测

## 19.4 示例

### 19.4.1 基于搜索的结构化预测：一阶依存句法分析

