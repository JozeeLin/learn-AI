# 第六章 文本特征构造

> 在语言处理中,向量x来源于文本数据,能够反映文本数据所具有的多种语言学特性.这种从文本数据到具体向量的映射称为"特征提取"和"特征表示",通过"特征方程"所完成.决定正确的特征是使一个机器学习项目取得成功的一部分.
>
> 深度学习网络减轻了对特征工程的需要,但是还是需要通过数值向量的形式来表示语言数据.
>
> 本章提供了一个当处理文本数据时能够作为特征的通用信息源的概述.



## NLP分类问题中的拓扑结构

> 通常来说,自然语言中的分类问题能够被分为几个宽泛的方向.其依赖于被分类的事项.
>
> 1. **词** 在这些问题中,我们面对的都是词(word).对于很多词来说,它们的解释依赖于其出现的上下文.
> 2. **文本** 在这些问题中,我们面对的都是一段文本,它可以是一个短语,句子,段落或一片文章.文本级别的NLP问题常见的为"文本分类问题".
> 3. **成对文本** 在这些问题中,会给定一对词或文本,然后需要了解成对的信息.相关的问题有:A和B是同义词吗?A是B的一个有效的翻译吗?文本A和B是被同一个作者所写的吗?句子A的含义能否通过句子B的推断.
> 4. **上下文中的词** 这些问题中,首先会有一段文本,其中包含一个特殊的词(短语,字符等).相关的问题有:在句子"I want to book a flight"中,对book这个词进行分类来判断,它是否是名词,动词还是副词,这些问题经常出现于为其他重要目标服务的上下文中,例如标记一个句子中的词语的词性,将一篇文本分割为句子,找到文本中所有的命名实体,找到所有提及同一实体的文本,等等.
> 5. **词之间的关系** 在一篇长文本的上下文中给定两个词或短语,然后需要对它们之间的关系进行陈述.例如,词语A是否是动词B的主语.



## NLP问题中的特征

> 特征通常表现为标量(indicator)和可数(count)的形式.
>
> 一个标量特征经常取0或1值,其取决于某种条件是否出现(比如,当dog这个词在文本中出现,特征取1,否则取0).
>
> 一个可数特征的取值取决于给定一个事件出现的频率(比如,以'dog'在文本中出现的次数作为特征值).

### 直接可观测特征

#### **单独词特征** 

被关注的词独立于上下文的时候,我们主要的信息来源是组成词的字符和它们的次序,以及从中导出的属性.另外,还应该着眼于词与其他外部信息资源的联系:词在文本集合中出现了多少次?词是否出现于美国的常用名列表中?等等.

**词元和词干**

将词语的不同形式(例如booking,booked,books)映射到它们的通用词元book.<u>这种映射经常由词元集或者形态分析器完成</u>.词的词元是歧义的,当词出现在上下文中,其词元化能够变得更加准确.

**词元化**是一个语言学定义过程,其对于不出现于词元集中的词或是拼写错误的词并不能很好的进行处理.

**词干提取(比词元化更粗糙的过程)**,它能够在任何字符串序列上起作用,以特定语言的启发式规则将词序列映射为更短的序列,以至于将不同的词语形式映射为相同的序列.<u>值得注意的是,词干提取的结构不需要是一个有效的词,例如"picture","pictures",'pictured'都会被词干化为"pictur";**也就是说,词干提取结果不唯一**.</u>

**词典资源**

记录词的语义资源.

举例来说:

> **英文词典资源WordNet**
>
> WordNet是一个非常大规模的人工构建的数据集,其尝试捕捉有关于词的概念语义知识.每个词均属于一个或多个同义词集(synset).每一个同义词集描述一个概念.
>
> 比如,'star'作为一个名词属于同义词集"astronomical celestial body, someone who is dazzlingly skilled, any celestial body visible from earch"和"an actor who plays a principle role".
>
> 同义词集互相之间通过语义关系相连,即上位关系和下位关系(更具体的或欠具体的词语).
>
> 其他的在WordNet中的语义关系包含反义和整体以及部分.WordNet包含与名词,动词,形容词和副词有关的信息.
>
> FrameNet这些词典重点围绕动词.列举了持有同一论元的动词(例如,'giving'的核心论元有Donor(捐赠者),Recipient(接收者)和Theme(被给予的东西)).
>
> PPDB是一个大型的自动构建的有关复述的数据集.

**分布信息**

另一个有关于词的资源就是分布(distributional),即哪些词与当前词的行为是类似的?

#### 文本特征

观察到的特征是字符和词在文本中的数量和次序.

**词袋** 

从句子和文本中抽取特征的过程是词袋(bag-of-words)过程(BOW).观察词在文档中出现的柱状图,即考虑每个词作为特征的数量.

**权重**

结合基于外部信息的统计结果,集中考虑那些在给定的文本中经常出现的词,并且它们在外部文本中出现的次数相对较少(在外部文本中出现的次数较多的词,通常为停用词(stopword,如,a,the等)).

当使用BOW方法时,经常使用TF-IDF权重.这种方法加大了那些能够有效区分当前文档的词的重要性.

####上下文词特征

当提取词在句子和文本中的信息时,一个能够直观观测到的词的特征就是其在句子中的位置,围绕它的词和字符也可作为特征.<u>**与目标词越近该词所具有的信息量相对于远处的词越丰富**</u>

**窗口**

基于上述词之间相互影响的原因,可以使用围绕词的窗口聚焦于词的直接上下文(即目标词每侧的k个词,k可设为2,5,10),之后使用特征来代表出现在窗口内的词.<u>**窗口方法是BOW方法的一种版本,但是其受限于小窗口**</u>

**位置**

除了词的上下文,我们还会对词在句子中的绝对位置感兴趣.如:类似于"目标词是句子中的第5个词"的特征

#### 词关系特征

考虑上下文中的两个词时,除了每个词的位置和围绕它们的词外,还能够观察词之间的距离和它们之间的代表词.

### 可推断的语言学特征

自然语言中的句子除了是词语的线性排序外还是有结构的.这种结构遵循复杂的**不易于直接观察到的规律**.

这些规律被归类为语法.在自然语言中针对这些规则和规律的学习被称为面向学习的语言学.部分控制语言的现象被很好地记录并被充分理解,这其中包括了例如词类(词性标签),形态学,语法乃至部分语义信息.

存在专门的系统以不同的准确率来预测词性标签,语法树,语义角色,篇章关系和一些其他语言学属性.<u>**这些预测能够作为有效的特征用于更进一步的分类问题**</u>

#### 语言学标注

**成分树**,也叫短语结构树.成分树是嵌套的,以括号表示语法单元的层次结构.

**依存树**,句子中的每个词是目标词的修饰,该目标词称为根节点.依存关系属于句法,与句子的结构有很大的关系.

成分树将词清晰地组成短语后,依存树能够清晰的展示修饰关系和词之间的连接.那些在句子中的外在形式不同(词性)的词语可能在依存树中会拥有直接的依存连接.

**回指消解(指代消解)**,用于解决代名词真正指代的是什么的问题.

**篇章关系**用于揭示句间的关系,例如解释(Elaboration),相对(Contradiction),因果(CauseEffect)等.这些关系经常被句间的连接词所揭示,例如"并且""然而"以及"和",同时也会被不直接的线索所揭示.

> 词性标签(成分树),句法角色(依存树),篇章关系,回指消解等概念是基于语言学理论的.其目标在于获得混乱的人类语言系统中的规律和规则.
>
> **语言学概念是否需要?**,一些深度学习的倡导者争论道:这种推断的,人为定义的语言学属性是没有必要的,神经网络是可以学习到这些中间表示的(或等价的,或更好的).
>
> 这里分两种情况来考虑,对于使用深度学习来学习语言学特征这个方法,它要求足够多的数据.



### 核心特征与组合特征

**线性模型**不能够为一个联合事件赋予一个值(因为这个联合事件其实是特征的组合,需要对这些特征分别赋值),除非联合事件本身作为一个特征.<u>因此,当为线性模型设计特征时,我们不仅需要定义核心特征,也需要定义很多组合特征.</u>

**神经网络**是非线性模型,并不会遇到这个问题.模型设计者能够金指定核心特征集合,然后依赖网络训练过程取选择重要的组合,这很大地简化了模型设计者的工作.

但是,在很多其他案例中,结合了有效的人工构造特征的线性分类器很难被击败,即使是使用核心特征的神经网络也只是接近了线性模型的性能.

### n元组特征

在给定长度下由连续的词序列组成.

### 分布特征

语言的分布假设由Firth[1957]和Harris[1954]提出,它们陈述了词的含义能够从它的上下文推断出来.通过观察词在一段文本中出现的模式,能够发现burger出现的上下文和pizza出现的上下文是相似的.

**利用这个特性,通过词出现的上下文去学习词的归一化;这些方法可以被归结为基于聚类的方法,其将相似的词归类到一个类别中,然后以其类别成员属性代表每个词**

**词嵌入方法**是另外一种类似的方法,它将每个词表示为一个向量,这样相似的词(拥有相似分布的词)会有相似的向量.

