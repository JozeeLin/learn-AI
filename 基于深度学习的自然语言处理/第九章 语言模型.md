# 第九章 语言模型

语言模型是给一个句子分配概率的任务.语言模型也对给定单词在一个词序列之后出现的可能性分配概率.

语言模型在现实世界中的应用中也是一个决定性的组件.例如机器翻译和自动语音识别,其中系统会产生一些翻译或转写,之后将**通过语言模型进行打分**.

正式来讲,语言模型就是给任何词序列$w_{1;n}$分配一个概率,也就是$P(w_{1;n})$.通过概率的**链式法则**则可以写成如下形式:
$$
P(w_{1;n}) = P(w_1)P(w_2|w_1)P(w_3|w_{1;2})P(w_4|w_{1;3})...P(w_n|w_{1;n-1})
$$
这是一系列的词预测任务,其中预测的每个词都取决于其前面的词.

语言模型使用**马尔科夫假设**,该假设规定未来的状态和现在给定的状态是无关的.形式上,一个k阶马尔科夫假设假设序列中下一个词只依赖于其前k个词:
$$
P(w_{i+1}|w_{1;i}) \approx P(w_{i+1}|w_{i-k;i})
$$
句子概率的估计就变成:
$$
P(w_{1;n}) \approx \prod_{i=1}^n P(w_i|w_{i-k;i-1})
$$
其中$w_{-k+1},...,w_0$被定义为特殊的补齐符号.(因为$w_0$前面没有词序列了).

接下来的任务就是根据给定的大量文本准确地估算出$P(w_{i+1}|w_{i-k;i})$.

这一章讨论了k阶语言模型.在第14章讨论未使用马尔科夫假设的语言建模技术.

## 语言模型评估:困惑度

有几种评价语言建模的度量.

1. 以应用为中心的度量方法通过在更高级别的任务中的性能来进行评价.

   > 例如,当将翻译系统中的语言模型组件从A替换为B后,测量翻译质量提高的程度.

2. 更直观的评估方法是对于未见的句子使用困惑度(perplexity).困惑度是一种信息论测度,用来测量一个概率模型预测样本的好坏,**困惑度越低越好.**

   > 给定一个包含n个词的文本语料$w_1,...,w_n$(n可以数以百万计)和一个基于词语历史的用于为词语分配概率的语言模型函数LM,LM在这个语料的困惑度是:
   > $$
   > 2^{-\frac{1}{n}\sum_{i=1}^n\log_2\rm{LM}(w_i|w_{1;i-1})}
   > $$
   > **好的语言模型将会为语料中的样例分配更高的概率,也会有更低的困惑度值**.
   >
   > 困惑度是与语料有关的---两种语言模型**只有在使用相同评价语料的情况下才可以比较困惑度**.
   >
   > <u>但困惑度不是一个很好的用于去评估语言理解或者语言处理任务的度量.</u>

## 语言模型的传统方法

对于k阶马尔科夫性质,$P(w_{i+1}=m|w_{1:i}) \approx P(w_{i+1}=m|w_{i-k:i})$,语言模型的任务是提供一个对$\hat{P}(w_{i+1}=m|w_{i-k:i})$的良好估计.这个估计通常由语料统计得到.

$\#(w_{i:j})$表示$w_{i:j}$在语料中出现的数目.$\hat{P}(w_{i+1}=m|w_{i-k:i})$的最大似然估计(MLE)是:
$$
\hat{P}_{MLE}(w_{i+1}=m|w_{i-k:i}) = \frac{\#(w_{i-k:i+1})}{\#(w_{i-k:i})}
$$
这种传统方法尽管有效,但很大的不足:如果序列$w_{i-k:i+1}$从未在语料中观察到(即$\#(w_{i-k:i+1}=0)$),模型分配的概率也会是0;**0概率会造成非常大的困惑度**.

> 一种避免0-概率时间的方法是使用**平滑技术**,确保为每个可能的情况都分配一个概率(可能非常小).
>
> - 添加平滑技术,也叫作加add-$\alpha$平滑
>
>   它假设每个事件除了语料中观测的情况外,至少还发生了$\alpha$次.这个估计就变成了:
>   $$
>   \hat{P}_{\rm{add}-\alpha}(w_{i+1}=m|w_{i-k:i}) = \frac{\#(w_{i-k:i+1})+\alpha}{\#(w_{i-k:i})+\alpha |V|}
>   $$
>   其中,|V|是词表大小,$0 < \alpha \le 1$.
>
> - 退避(back-off)
>
>   如果没有观测到k元文法,那么就基于(k-1)元文法计算一个估计值.代表性例子是**贾里尼克插值平滑**.
>   $$
>   \hat{P_{\rm{int}}}(w_{i+1}=m|w_{i-k:i}) = \lambda_{w_{i-k:i}}  \frac{\#(w_{i-k:i+1})}{\#(w_{i-k:i})} + (1-\lambda_{w_{i-k:i}})\hat{P_{\rm{int}}}(w_{i+1}=m|w_{i-(k-1):i})
>   $$
>
> - 改进的Knerser Ney平滑技术

### 延伸阅读

Michael Collins的上课笔记中可以找到语言模型任务的很好的正式描述和困惑度背后的动机.

> http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf

平滑技术的良好概述和经验评估可以在Chen和Goodman[1999]以及Goodman[2001]的工作中找到.

可以在Mikolov博士论文(Mikolov[2012])中的背景章节找到传统语言模型技术的综述.

对于前沿非神经网络语言模型的介绍可以查看Pelemans等[2016].

### 传统语言模型的限制

基于最大似然估计(MLE)的语言模型很容易训练,可扩展到大规模语料,实际应用中表现良好.然而,它有几个重要的缺点.

退避法的缺点在于不够灵活,因为它需要事先假设一个固定的必须人工设计的退避规则.(例如,如果想要基于前k个词并且基于文本类型设置条件,退避法是应该首先抛弃前第k个词还是类型变量?).

基于最大似然估计的语言模型存在稀疏问题.它使得该算法很难在更大的上下文条件语境中有效.

最终,基于最大似然估计的语言模型缺乏对上下文的泛化.比如,观察到black car和blue car并不会影响我们估计出现red car的概率,如果我们之前没有观测到它.<u>基于类的语言模型试图公国分布式算法进行词聚类来解决这个问题,以导出此类代替或作为单词的附属成为条件.</u>

## 神经语言模型

非线性神经网络语言模型可以解决一些传统语言模型中的问题:它可以在增加上下文规模的同时参数仅呈线性增长,缓解了手工设计退避规则的需要,支持不同上下文的泛化性能.

<u>本节介绍的这种形式的模型由bengio等[2003]推广</u>

神经网络的输入是k元文法$w_{1:k}$,输出是下一个词的概率分布.k个上下文词$w_{1:k}$被当做一个单词窗口;每个词w和词嵌入$v(w) \in R^d$对应,输入向量x是k个词的串联拼接.
$$
x  = [v(w_1);v(w_2);...;v(w_k)]
$$
输入的x之后被传给一个拥有一个或多个隐层的多层感知器(MLP):
$$
\begin{aligned}
\hat{y} = P(w_i|w_{1:k}) = \rm{LM}(w_{1:k}) &= \rm{softmax}(hW^2+b^2) \\
h &= g(xW^1+b^1) \\
x &= [v(w_1);v(w_2);...;v(w_k)] \\
v(w) &= E_{[w]}
\end{aligned}
$$
V是一个有限的词表,包括未登录词标识UNK,句子开头的补齐符号标识\<s\>,以及序列结尾的标识\</s\>.

### 训练

训练样本:语料中的k元文法,其中前k-1个词被用作特征,最后一个单词被用作分类的目标标签.

损失函数:交叉熵损失函数(但需要使用昂贵的softmax操作,这对于非常大的词表可能是难以承受的).

### 内存和计算效率

k个输入单词中的每个单词都为x添加了$d_w$维,从k个词变到k+1个词将会增加权重矩阵$W^1$的维度,<u>维度从$k.d_w \rm{x} d_{hid}$增加到$(k+1).d_w \rm{x} d_{hid}$,</u>和基于传统方法的多项式增加相比,这在参数数量上是很小的线性增加.

词表中的每个词都对应于一个$d_w$维度的向量(E中的一列)和一个$d_{hid}$维度的向量($W^2$中的一行).

### 大规模输出空间

使用具有大规模输出空间的神经概率语言模型(即有效地在整个词表上计算softmax)在训练时间和测试时间上都是不可行的.解决方案如下:

- **层次化softmax**[Morin and benjio 2005]允许计算O(log |V|)中单个词的概率而不是O(|V|)

  这可以通过将softmax的计算构建成遍历树的形式得到,每个词的概率作为树的枝干被选择的乘积.

- **自归一化方法**例如噪声对比估计(NCE)[Mnih and Teh, 2012, Vaswani et al., 2013]或者在训练目标中增加归一化项.

  NCE方法通过使用一系列**二分类问题替换交叉熵目标函数提升了训练时间效率**,需要评估对于k个随机词而不是整个词表的分配数值.也可**通过将模型推向生产"近似归一化"指数得分来提高测试时间**,是模型得分为一个单词出现概率的良好替代.

- 另一类独立的工作是尝试使用字符级来回避问题,而不是词语级.

> 一个不错的综述和将这些方法与其他处理大输出词表的技术的对比参见Chen等[2016]

### 期望特性

只考虑条件上下文的一部分和生成未见上下文的灵活性,以及内存与计算方面仅线性依赖于有影响的上下文规模,这些使得调节上下文的大小变得非常容易,不会因数据稀疏和计算效率而受损.

神经语言模型可以轻松融入较大而灵活的条件上下文并允许对上下文自由定义.例如,Devlin等[2014]提出了一种机器翻译模型,其中下一个词的概率由已经生成的前k个词和在源语言中的m个词所决定,这m个词在翻译中的位置是给定的.这允许模型对特定主题的术语和源语言中的多词表达式更加敏感,它实际上很大地提高了翻译成绩.

### 局限

- 预测上下文中一个词的概率比使用传统语言模型代价更高,使用大规模词典和训练语料也变得难以接受.但它们能够更好的利用数据,甚至在相对较小的训练集上取得很有竞争力的困惑度.

传统语言模型与神经语言模型结合时,翻译质量确实会提高;神经语言模型对于未见的事件泛化性能更好,但是有时这种泛化会损失性能,而刚性的传统方法会更好一些.

> 参见Pelemans等[2016]和其中的文献,查看相关与使用skip-gram用于探索非神经语言模型.

## 使用语言模型进行生成

语言模型也可用于生成句子.

###编码(训练)

> 在给定文本集合上训练语言模型之后,可以使用以下过程根据其概率从模型生成("抽样")随机句子:
>
> 预测基于起始符号的第一个词的概率分布.并根据<u>预测分布</u>绘制一个随机词.然后,预测基于第一个词的<u>第二个词的概率分布</u>.
>
> 如此预测后面的分布,直到预测到序列结束的符号\</s\>.

### 解码(生成)

当从一个使用这种方式进行训练的语言模型中进行解码(生成一个句子)的时候,可以在每个步骤中**选择得分最高的预测(单词)**,或者**根据预测分布随机采样出一个字**,或者**使用beam Search来查找具有全局高概率的序列**.

> 第一种方式在每个步骤均选择最高预测可能导致次优的总体概率,它有可能"使自己陷入困境",从而导致和面是低概率事件.**这被称为标签偏置问题**
>
> Andor等人[2016]和Lafferty等人[2001]有深入的讨论.



## 副产品:词的表示

矩阵$W^2$中的每一列都是一个$d_{hid}$维的向量,该矩阵中的每一列都会乘以文本表示$h$,通过这种方式可以得到相关词项的分数.**直观地,这应该导致在相似的上下文中出现具有相似向量的单词.**

> 根据分布假设,出现在相似语境中的词具有相似的含义,具有相似意义的词将具有相似的向量.<u>这里可以通过对词嵌入矩阵E来进行验证.</u>

在矩阵E和$W^2$的行和列中学习有用的词表示.