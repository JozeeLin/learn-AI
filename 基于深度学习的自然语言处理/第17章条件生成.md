# 第17章条件生成

如14章所述，RNN可以用于基于全部历史信息条件下的**非马尔可夫(non-markovian)语言模型**。

这种能力使得RNN适合作为**自然语言生成器**以及在复杂输入条件下的**条件生成器(conditioned generator)**。

本章将对这些结构进行讨论。

## 17.1RNN生成器

> 如9.5节所述，任何语言模型都能够用于生成。

对于RNN转换器，生成过程是通过将i时刻转换器的输出作为i+1时刻的输入来完成的：在预测出下一个输出符号的概率分布$p(t_i=k|t_{1:i-1})$之后，选择一个符号$t_i$和相应的嵌入向量(embedding vector)作为下一时刻的输入。当一个特定的序列结束符(通常记为</s\>)被产生时，生成过程即停止。

在使用一个训练好的RNN转换器进行生成时，1.选择每一步概率最高的项进行输出；2.根据模型预测的分布进行采样，或者使用beam search(柱搜索)来获得一个全局高概率的输出。

**利用基于字符级别RNN模型所生成的文本不仅与流利的英文文本类似，还表现出对n元语法语言模型无法捕获的一些性质的敏感性，保留序列的长度以及嵌套括号的匹配**。

> 基于RNN的字符级别语言模型性质的一个有趣的演示以及分析，请参考Karpathy等【2015】

### 17.1.1 生成器的训练

**一般情况下，**简单地将其当做一个转换器来进行训练，使得在给定序列中已观测元素的条件下，下一个观测元素具有高的概率(即以语言模型的方式训练)。

**更具体的，**对于训练语料库中长度为n的句子$w_1,...,w_n$，我们生成一个具有n+1个输入以及n+1个相应输出的RNN转换器，其中第一个输入是句首符，接下来是句子的n个词。第一个期望的输出是$w_1$，第二个期望输出是$w_2$，以此类推，第n+1个期望输出是句尾符。

**注意：**该生成器以实际观测到的词作为每一时刻的输入，即便该词在当前模型预测下的概率较低；而在测试阶段，该状态下本应生成一个不同的词。

## 17.2条件生成（编码器-解码器）

该生成框架基于已生成的词项$\hat{t}_{1:j}$，来生成下一个词项$t_{j+1}$:
$$
\hat{t}_{j+1} \sim p(t_{j+1}=k|\hat{t}_{1:j}) \tag{17.1}
$$
在RNN的框架中则根据下式进行建模：
$$
\rm{p(t_{j+1}=k|\hat{t}_{1:j}) = f(RNN(\hat{t}_{1:j}))} \tag{17.2} \\
\hat{t}_j \sim p(t_j|\hat{t}_{1:j-1})
$$
使用递归定义则表示为：
$$
\begin{aligned}
p(t_{j+1} &= k|\hat{t}_{1:j} = f(O(s_{j+1}))) \\
\textbf{s}_{j+1}&= R(\hat{t}_j,s_j) \\
\hat{t}_j &\sim p(t_j|\hat{t}_{1:j-1})
\end{aligned}
$$
R是递归子结构函数。

其中f是一个参数化函数，将**RNN状态映射为此的概率分布**。比如softmax。

在条件生成的框架中，下一个词项的生成依赖于已生成的词项以及一个额外的条件上下文c。
$$
\hat{t}_{j+1} \sim p(t_{j+1}=k|\hat{t}_{1:j},c) \tag{17.4}
$$
使用RNN框架时，该上下文c被表示为向量$\textbf{c}$：
$$
\begin{aligned}
p(t_{j+1}=k|\hat{t}_{1:j},c) &= \rm{f(RNN(v_{1:j}))} \\
\textbf{v}_i &= [\hat{t}_i;c] \\
\hat{t}_j &\sim p(t_j|\hat{t}_{1:j-1},c)
\end{aligned}
$$
使用递归定义则表示为：
$$
\begin{aligned}
p(t_{j+1}=k|\hat{t}_{1:j},c) &= \rm{f(O(s_{j+1}))} \\
s_{j+1} &= R(s_j,[\hat{t}_i;c]) \\
\hat{t}_j &\sim p(t_j|\hat{t}_{1:j-1},c)
\end{aligned}
$$
在生成过程中的每个阶段，上下文向量c与输入向量$\hat{t}_j$进行拼接，并将拼接之后的向量作为RNN的输入，从而得到下一个预测。

**哪一类信息能够被编码至上下文c中呢？**几乎任何我们能够获得并且任务有用的数据都可以。

### 17.2.1 序列到序列模型

在序列到序列的条件生成中，给定一个源序列$x_{1:n}$(例如法语中的一个句子)时，生成一个目标输出序列$t_{1:m}$(例如法语句子对应的英文翻译)。通过使用一个编码函数$\rm{c=ENC(x_{1:n})}$——通常是一个RNN。$\rm{c=RNN^{enc}(x_{1:n})}$——将源句子$x_{1:n}$编码为一个向量来实现的。

接下来，使用一个条件生成器RNN(解码器)并根据式(17.5)来生成期望的输出序列$t_{1:m}$。

**训练**

用于编码器与解码器的RNN是联合训练的。**监督信号**只出现在解码器RNN端，但是梯度能够沿着网络连接反向传播至编码器RNN中。

### 17.2.2 应用

**机器翻译** 使用深度LSTM循环神经网络的序列到序列方法已被证明对于机器翻译任务惊人地有效[Sutskever et al.,2014]。**trick：将源句子倒序输入**，使得$x_n$对应于输入句子的第一个词。<u>在这种方式下，第二个RNN（解码器）能够更容易地建立源句子首词语目标句子首词之间的联系。</u>

> 在17.4节，将会介绍基于**注意力机制结构**，这是一种在序列到序列结构上的精巧设计，对于机器翻译更为有效。

**邮件自动回复** 该任务旨在将可能较长的邮件文本映射至一条较短的回复。更多细节请参考Kannan等人[2016]。

**形态屈折(morphological inflection)**在此任务中，输入是一个基本词(base word)以及一个期望的形态变化需求，输出是该词的屈折形式。

**其他应用** Filippova等人[2015]使用该结果进行删除式的句子压缩。Glillick等人[2016]把词性标注与命名实体识别任务看作一个序列到序列的问题。Vinyals等人[2014]把句法分析当作一个序列到序列的任务，将一个句子映射为一系列的短语结构加括决策。

### 17.2.3 其他条件上下文

事实上，条件上下文向量可以是基于单个词，或一种连续词袋(CBOW)的编码，也可以由一个卷积网络生成，或基于一些其他的复杂计算。

> 在对话任务的设定下(该任务中我们训练一个RNN来对对话中的消息产生回复)。Li等人[2016]使用一个与用户(当前回复的作者)冠梁的可训练的嵌入向量作为上下文。<u>其背后的直觉是不同用户由于年龄、性别、社会角色、背景知识、个性特点以及其他很多潜在因素的不同而有着不同的交流风格。</u>

除了自然语言之外，另一种流行的应用是图像描述生成(image captioning)，更多请参考[Karpathy and Li,2015,Mao et al.,2014,Vinyals et al.,2015]。

Huang等人[2016]的工作将图像描述生成任务扩展为视觉故事讲述(visual story telling)，在该任务中，输入是一系列的图像，输出是对图像演变进行描述的一个故事。<u>该任务中采用的是RNN作为编码器来读取输入图像向量所构成的序列。</u>

## 17.3 无监督的句子相似性

**基于序列到序列框架，首先训练一个RNN编码器来产生上下文向量表示c，该向量将用于一个RNN解码器来完成某个任务。**

### 17.3.1 自动编码

自动编码(auto-encoding)方法是一种条件生成模型，该模型首先使用RNN对一个句子进行编码，然后由解码器试图对该输入句子进行重建。

模型将被训练句子对于重建句子所需要的信息进行编码，同时期望相似的句子具有相似的向量。

### 17.3.2 机器翻译

该方法需要一个用于条件生成任务的大语料库，比如机器翻译中所使用的平行语料库。

### 17.3.3 skip-thought

Kiros等人[2015]所描述的模型——对于句子相似度问题提出了一个有趣的目标函数。该模型将词语的分布假设延展到句子。认为出现在相似上下文的句子是语义相似的(在这里，一个句子的上下文指的是围绕该句子的其他句子)。

skip-thought模型是一个条件生成模型，其中，一个RNN编码器将句子映射为一个向量，然后一个解码器RNN被训练以根据该编码向量来**重建该句子的前一个句子**，另一个解码器RNN被训练以根据该编码器向量来**重建该句子的后一个句子**。

### 17.3.4 句法相似度

Vinyals等人[2014]的工作表明编码器-解码器模型能够在基于短语结构的句法分析任务上取得不错的结果，具体方式是**先对句子进行编码，而解码器则将生成一系列加括(bracketing)决策来重建一颗线性化的句法分析树**。

## 17.4 结合注意力机制的条件生成

前面的encoder-decoder模型，是**基于一个强假设**之下的，即强制编码器所得到的向量$\rm{c=RNN^{enc}(x_{1:n})}$中包含生成时所需要的全部信息，并且要求生成器能够从该定长向量中提取出所有信息。

**结合注意力机制的条件生成结构放宽了在简单条件生成结构中全部源句子信息被编码为单一向量的条件**，而是使用一组向量来表示源句子，同时解码器采用一种软注意力机制(soft attention mechanism)来决定编码的输入序列中将关注哪些部分。

**更具体的，**结合注意力机制的编码器——解码器结构使用一个biRNN对长度为n的输入序列$x_{1:n}$进行编码，产生n个向量$c_{1:n}$：
$$
\textbf{c}_{1:n} = \rm{ENC(x_{1:n})=biRNN^*(x_{1:n})}
$$
接下来，生成器(解码器)将这些向量当作一段只读记忆，用来表示输入的条件句：在生成过程的每个步骤j，它将会从$c_{1:n}$中选择哪些向量进行关注，从而得到一个含焦点的上下文向量$c^j = \rm{attend}(c_{1:n},\hat{t}_{1:j})$。

**$c_j$将被用作第j步生成过程的条件：**
$$
\begin{aligned}
p(t_{j+1}=k|\hat{t}_{1:j},x_{1:n}) &= f(O(s_{j+1}))  \\
s_{j+1} &= R(s_j, [\hat{t_j};c^j])\\
c^j &= attend(c_{1:n},\hat{t}_{1:j})\\
\hat{t}_j &\sim p(t_j|\hat{t}_{1:j-1},x_{1:n})
\end{aligned}
\tag{17.7}
$$
**函数attend的形式是什么呢？**(**注意：**这里的注意力机制是软注意力机制)

形式化地，在步骤j通过软注意力机制得到一个加权平均向量$c^j$:
$$
c^j = \sum_{i=1}^n \alpha^j_{[i]}.c_j
$$
$\alpha^j \in \mathbb{R}^n_+$是步骤j的注意力权重向量，其中$\alpha^j_{[i]}$都是正值且和为1。

$\alpha^j_{[i]}$经过两步得到:

1. 首先，根据解码器第j步的状态以及编码器的每一个向量$c_i$，使用一个前馈神经网络$\rm{MLP^{att}}$得到非归一化的注意力权重$\bar{\alpha}^j_{[i]}$:
   $$
   \begin{aligned}
   \bar{\alpha}^j &= \bar{\alpha}^j_{[1]},...,\bar{\alpha}^j_{[n]} = \\
   &=MLP^{att}([s_j;c_1]),...,MLP^{att}([s_j;c_n])
   \end{aligned}
   \tag{17.8}
   $$

2. 接下来，使用softmax函数将权重$\bar{\alpha}^j_{[i]}$归一化至一个概率分布:
   $$
   \alpha^j = softmax(\bar{\alpha}^j_{[1]},...,\bar{\alpha}^j_{[n]})
   $$

   > 在机器翻译任务中，可以将$MLP^{att}$理解为计算当前解码器状态$s_j$与源句子每个部分$c_i$的一种软对齐(soft alignment)。

那么，完整的表示形式为：
$$
\begin{aligned}
attend(c_{1:n},\hat{t}_{1:j}) &= c^j \\
c^j &= \sum_{i=1}^n \alpha^j_{[i]}.c_i\\
\alpha^j &= softmax(\bar{\alpha}^j_{[1]},...,\bar{\alpha}^j_{[n]}) \\
\bar{\alpha}^j_{[i]} &= MLP^{att}([s_j;c_j])
\end{aligned}
\tag{17.9}
$$
完整的结合注意力机制的序列到序列生成则由以下式子进行计算：
$$
\begin{aligned}
p(t_{j+1}=k|\hat{t}_{1:j},x_{1:n}) &= f(O(s_{j+1}))  \\
s_{j+1} &= R(s_j, [\hat{t_j};c^j])\\
c^j &= \sum_{i=1}^n \alpha^j_{[i]}.c_i\\
\textbf{c}_{1:n} &=biRNN^*(x_{1:n})\\
\alpha^j &= softmax(\bar{\alpha}^j_{[1]},...,\bar{\alpha}^j_{[n]}) \\
\bar{\alpha}^j_{[i]} &= MLP^{att}([s_j;c_j]) \\
\hat{t}_j &\sim p(t_j|\hat{t}_{1:j-1},x_{1:n}) \\
f(z) &= softmax(MLP^{out}(z)) \\
MLP^{att}([s_j;c_i]) &= v \tanh([s_j;c_i]U+b)
\end{aligned}
\tag{17.10}
$$

> 为什么要使用biRNN编码器将条件序列$x_{1:n}$编码成上下文向量$c_{1:n}$，而不将注意力机制直接作用于$x_{1:n}$？
>
> 原因：
>
> 1.首先，上下文向量$c_i$表示的是聚焦在输入词项$x_i$周围的上下文窗口，而不只是$x_i$本身。
>
> 2.其次，编码器与解码器可以协同演化，同时该网络可以学习如何对输入序列中有助于解码的属性进行编码。

### 17.4.1 计算复杂性

O(mxn)，时间消耗在解码阶段。

### 17.4.2 可解释性

- 对于编码向量中究竟编码了什么信息？
- 解码器是如何利用这些信息的？
- 导致解码器特定行为的原因是什么？

含注意力的结构的一个重要好处是它提供了一种简单的方式使得我们能够一窥解码器的推理过程以及模型究竟学习到了什么。

解码过程的每一步中，可以根据注意力权重$\alpha^j$来观察在产生当前输出时解码器任务源序列中哪些区域是相关的。

## 17.5 自然语言处理中基于注意力机制的模型

结合注意力机制的条件生成是一种非常强大的结构。是目前最好的机器翻译系统所使用的主要算法。

### 17.5.1 机器翻译

**子词单元**

- 目的：限制词表大小

  为了处理高度屈折语言的词表(为了通用意义上对词表大小进行限制)

- 解决思路

  提出使用比词小的子词单元(sub-word unit)

- 算法原理

  - 在处理源端与目标端文本时，使用一种称为BPE的算法来寻找典型的子词单元(10.5.5节末尾描述了此算法)。

  - 然后，将归纳得到的词语切分模型对源句子与目标句子进行分词处理。
  - 经过处理的语料作为输入，对一个含注意力的seq2seq网络进行训练
  - 输出序列经过一次处理，以将子词单元重新合并为词。

  > 该过程减少了未登录词的数目，使得模型更容易泛化至新词，并且提高翻译质量。



**融合单语数据**

- 目的：训练数据增强

- 解决思路

  **传统统计翻译系统：**1.先在双语平行数据上训练一个翻译模型，2.在一个更大规模的单语数据上训练一个单独的语言模型。

  **seq2seq翻译系统：**1.首先，训练一个从目标端到源端的翻译模型，2.使用模型对大量的目标端句子进行翻译，3.将所得到的(目标，源)句对作为(源，目标)实例添加至平行语料中，4.在合并之后的语料上训练一个源端到目标端的机器翻译系统。

**语言学标注**

- 目的：增加输入向量的信息
- 解决思路：通过一个语言学标注的流水对其进行词性标注、句法依存分析以及词形还原。接下来，每个词的信息将增强其词性标记($p_i$)，依存标签($r_i$)、词的原形$l_i$以及形态学特征$m_i$的编码向量。
- 效果说明：以上解决思路表明语言学信息即使在所使用的模型非常强大且在理论上能够学到这些语言学信息的条件下，也能够发挥作用。

**开放问题**

神经机器翻译中的主要开放问题：

1. 扩大输出词表的大小(或者通过采用基于字符的输出来消除词表大小的依赖)
2. 训练过程考虑beam search
3. 加速训练和编码过程
4. 利用句法信息的模型

### 17.5.2 形态屈折

通过构建一个由屈折类型信息以及书如此的字符列表所构成的输入序列，该问题可以由一个序列到序列的模型来进行建模。期望的输出则为目标词的字符列表。

### 17.5.3 句法分析

Vinyals等人[2014]的工作表明含注意力的序列到序列模型通过读入一个句子并输出一个加括的决策序列，能够取得有竞争力的句法分析结果。

