# 第11章 使用词嵌入

本章将讨论一部分词向量的用途

## 11.1 词向量的获取

从一个语料库中可以很容易地训练词嵌入向量，并且已有有效的训练算法实现。

> 注意：**训练机制和底层语料库的差异对最终的表示有很大的影响**，并且可用的预训练的表示可能不是特定应用的情况下的最佳选择。

目前word2vec的实现版本如下：

- c语言版https://code.google.com/archive/p/word2vec
- python版本https://radimrehurek.com/gensim
- 允许使用任意上下文信息的改进版https://bitbucket.org/yoavgo/word2vecf.

目前glove的有效实现如下：

- http://nlp.stanford.edu/projects/glove/

预训练词向量的来源：

- 英文词向量
  - word2vec项目和glove项目提供对应的预训练词向量
- 其他语言词向量
  - Polyglot项目中获得http://polyglot.readhedocs.org



## 11.2词的相似度

预训练词向量的用途：

- 将它们输入到神经网络中

- 使用相似函数计算两个词向量之间的相似度，**常见的相似函数为余弦相似度**
  $$
  \rm{sim}_{cos}(u,v) = \frac{u.v}{\|u\|_2\|v\|_2}
  $$


## 11.3词聚类

词向量可以通过一些聚类算法进行聚类，比如定义在欧式空间上的K均值方法。

## 11.4寻找相似词

使用行正则化嵌入矩阵，两个词$w_1$与$w_2$的余弦相似度由下式给出：
$$
\rm{sim}_{cos}(w_1,w_2) = E_{[w_1]}.E_{[w_2]}
$$


根据分布式方法得到的词相似度可以与其他形式的相似度进行结合。

- 基于拼写相似程度定义一种度量（共享相同字母的词语）

  > 通过过滤与目标词语分布式相似度排名前k的列表得到拼写上也相似的词，从而找到该目标词语的拼写变形和常见拼写错误。

**一组词的相似度**

对应的应用场景是：

- <u>通常在我们已经得到一个相关词列表并希望对词表进行扩充时出现（如已知4个国家的列表，希望扩充出更多的国名等其他场景）</u>

- 当我们将相似度视为对于一个词的给定含义的相似度的时候，通过创造一个与该含义相关的词列表，可以将查询词的相似度与这个含义关联。

  > 将词组与词w的相似度定义为：
  > $$
  > \rm{sim}(w,w_{1:k}) = \frac{1}{k} \sum_{i=1}^k sim_{cos}(w, w_i)
  > $$
  >



**计算一组词与其他多个词的平均余弦相似度**
$$
s = E(w_1+w_2+...+w_k)/k
$$

## 11.5同中选异

给定一个词列表，找出不属于这个列表的某个词，称为同中选异（odd-one-out)

1. 求出每个词与词列表的相似度
2. 返回一个相似度最小的词（最不相似的词）



## 11.6 短文本相似度

形式上，考虑两个文档$D_1 = w_1^1,w_2^1,w_3^1,...,w_m^1$和$D_2=w_1^2,w_2^2,...,w_n^2$，并将文档相似度定义为：
$$
\rm{sim}_{doc}(D_1, D_2) = \sum_{i=1}^m\sum_{j=1}^n \cos(w_i^1, w_j^2)
$$
使用基本的线性代数方法，假设以上的文档词向量为正则化词向量，那么上式表示成连续词袋的点积来计算：
$$
\rm{sim}_{doc}(D_1,D_2) = (\sum_{i=1}^m w_i^1).(\sum_{j=1}^nw_j^2)
$$
那么文档$D^{'}=w^{'}_{1:n}$与文档集合$D_{1:k}$的相似度计算：
$$
s = D.(\sum_{i=1}^n w_i^{'})
$$

## 11.7 词的类比

### 11.7.1 类比任务

$$
\rm{analogy(m:w\rightarrow k:?)} = \rm{argmax}_{v \in V;\notin \{m,w,k\}}\cos(v,k-m+w) \tag{11.4}
$$

Levy和Goldberg[2014]发现，对于正则化向量，解决等式11.4的极大化等价于解决等式11.5：
$$
\rm{analogy}(m:w \rightarrow k:?) = \rm{argmax}_{v \in V;\notin {m,w,k}}\cos(v,k)-\cos(v,m)+\cos(v,w) \tag{11.5}
$$
等式11.5称为3CosAdd方法。但此方法存在一个可能缺陷：由于目标的加和性质，和中的某一项可能会控制表达式，从而使得其他词的信息被严重忽略。

3CosMul方法可以解决这种缺陷：
$$
\rm{analogy}(m:w \rightarrow k:?) = \rm{argmax}_{v \in V;\notin {m,w,k}} \frac{\cos(v,k)\cos(v,w)}{\cos(v,m)+\epsilon}
$$

## 11.8改装和映射

Faruqui等人[2015]提出的改装方法允许使用这样的数据来提高词嵌入矩阵的质量，Faruqui等人[2015]展示了利用WordNet和PPDB的信息来提高预训练嵌入向量的方法的有效性（参考6.2.1节）

**假设**存在一个预训练词嵌入向量矩阵E和一个编码了词间二元相似度关系的图g——图中的结点是词，如果他们是有边直接连接的，那么词就很相似。

> 值得注意的是这里的图是一种一般化的表示，一个包含被认为彼此相似的词对的列表可以轻松地套用在该框架内。

这种方法的机制是解决一个优化问题：**搜索一个新的词嵌入矩阵$\hat{E}$使得其每一行和E中对应行接近的同时也接近于其在图g中的邻居。**

具体优化目标是：
$$
\rm{argmin}_{\hat{E}} \sum_{i=1}^n (\alpha_i \|\hat{E}_{[w_i]}-E_{[w_i]}\|^2 + \sum_{(w_i,w_j)\in g}\beta_{ij}\|\hat{E}_{[w_i]}-\hat{E}_{[w_j]}\|^2)
$$
其中，$\alpha_i$和$\beta_{ij}$反映了一个词和它本身或其他单词相似的重要程度。实际上，$\alpha_i$通常都设为1，$\beta_{ij}$设为词$w_i$在图G中度的倒数（如果一个词同时有几个近邻，则对于其中每个词的影响都会较小），在实际应用中这种方法的效果非常好。

**问题：一个词存在于两个嵌入矩阵中怎么处理？**

一个是从小词表中得到的$E^S$，另一个是从大词表中获得并单词训练的$E^L$。二者是不相容的。如果想要用大矩阵$E^L$中的词向量来表示小词表$E^S$中无法得到的词，可以通过**线性映射(必须基于假设前提：两个空间是可以线性对应的。)的方法**，在两个嵌入空间中架起“桥梁”[Kiros et al. ，2015，Mikolov et al. , 2013]，训练目标是寻找一个映射矩阵M来将$E^L$中的列与$E^S$中的列相对应。

具体的方法是解决以下优化问题：
$$
\rm{argmin_M} \sum_{w\in V_S \and V_L} \|E^L_{[w]}.M-E^S_{[w]}\|
$$
学习到的矩阵用于映射$E^L$中无法与$E^S$中相对应的列。<u>Kiros等人[2015]成功地使用这种方法提高基于LSTM的句子编码器的词表大小。</u>

## 11.9 实用性和陷阱

在使用预训练词向量需要考虑一下这些因素：

- 训练集的资源
  - 资源大小
  - 对应的领域
  - 特定问题
- 定义相似度分布的上下文内容
- 超参数

具体参考Levy等人[2015]的工作。

###**注意：**

1. 当使用预训练的词向量时，**最好使用与源语料相同的切分词项的方法和文本规范化方法**。
2. 由词向量引入的相似度问题是基于分布信号的，因此容易受到10.7节中所讨论的分布式相似度方法的所有限制的影响。**当使用词向量时需要考虑这些限制。**



y