# 第12章 案例分析：一种用于句子意义推理的前馈结构

【短文档的相似度基线】在11.6节中，介绍了将短文档中**成对词语的相似度之和当作它们的相似度的基线方法**。

定义如下：

> 给定两个句子，第一个由词$w_1^1,...,w^1_{l_1}$组成，第二个词$w^2_1,...,w^2_{l_2}$组成，每个词都与预训练过程中的对应词向量关联，两个句子的相似度定义为：
> $$
> \sum_{i=1}^{l_1}\sum_{j=1}^{l_2}\rm{sim}(w_i^1, w_j^2)
> $$
>

本章会展示**在有训练数据的情况下**，文档相似分数可以有多大提升。本章将采用Parikh等人[2016]针对斯坦福语言推理(Stanford Natural Language Inference, SNLI)中语义推理任务提出的网络模型。

这个模型说明了如何将到目前为止所描述的基本网络组件组合在不同的层中，从而形成一个复杂而又强大的网络，并针对某个任务进行联合训练。

## 12.1 自然语言推理与SNLI数据集

在自然语言推理任务即文本蕴含识别（Recognizing Textual Entailment，RTE）中会给出两个文本$s_1$和$s_2$，需要确定$s_1$和$s_2$是**蕴含**（也就是可以通过$s_1$推断出$s_2$)，**矛盾**（不可以同时为真）或**中立**（第二个既不与第一个相互矛盾，也无法通过第一个推理得到）。

> 蕴含任务是Dagan和Glickman[2004]提出的，随后由一系列为PASCAL RTE挑战的基准[Dagan et al. ,2005]建立起来。

对于该任务的深度讨论和非神经网络的解决方法，可以参考Dagan，Roth，Sammons和Zenzotto所著的书[Dagan et al.,2013].

> SNLI是一个由Bowman等人[2015]提出的大数据集，其中包含了57万人类手写的句子对，每对都由人工标注为蕴含、矛盾和中立。
>
> SNLI数据集是用于获取有意义的推理模型的常用数据集。需要注意的是该任务超出了仅仅是词对间的相似度。

**任务的问题描述：**

在此任务中我们需要强调一些相似度并减少其余相似度的能力，并且需要理解哪种相似度保留了原意(例如，在一个手术环境下的man到patient)，哪种添加新的信息(如，patient到man)。**网络结构就是设计用来帮助这种推理的**

## 12.2 文本相似网络

网络分几个阶段进行工作：

- 第一阶段，目标是计算句子对的相似度并且使其与任务相适应。两个词向量的相似度函数定义为：
  $$
  \rm{sim}(w_1,w_2) = \rm{MLP}^{transform}(w_1).MLP^{transform}(w_2)
  $$

  1. 首先用训练好的非线性变换把每个词转化为词向量
  2. 然后，取变换后向量的点积

  对于两个句子a和b，句子a中的词$w_i^a$在b中的相似度向量，通过softmax正则化后，得到该词对于句子b的**对齐向量**：
  $$
  \alpha^a_i = \rm{softmax}(\rm{sim}(w_i^a,w_1^b),...,\rm{sim}(w_i^a,w_{l_b}^b))
  $$
  使用词$w_i^a$的对齐向量作为权重向量，计算句子b词语列表的加权和：
  $$
  \bar{w_i^b} = \sum_{j=1}^{l_b} \alpha^a_{i[j]}w_j^b
  $$
  向量$\bar{w_i^b}$捕获了通过a中的第i个词触发的句子b中的单词的加权混合。

  一系列向量的这个加权和表示（其中的权重由等式(12.2)所示的得分使用softmax计算得出)通常称为**注意力机制**。

  > **命名的根据**：权重反映了目标序列中的每个项对给定源项的重要性——对于目标序列中的每个项，对于源项应给予多少注意力。



  <u>转化网络学习了一个词级别对齐的相似度函数。将每个词转化到保留了词级别相似性的空间中。</u>

  经过转化网络之后，两个词向量相近的词基本会**指向同一个实体或事件**。

  > **网络的目标是为了找到有助于蕴含的关键词。**
  >
  > 从两个方向得到对齐结果(软对齐方式)：
  >
  > 1. 将a中的每个词对齐到b中多个词语
  > 2. 对b中的词语也会找多个在a中对齐的词。

- 第二阶段，查看带权重的多个词单元并加入方向性

​	接下来，配对网络使用CBOW加权表示法查看每个对齐的对（词+组），并提取与该对相关的信息。

​	我们需要将每个这样的对($w_i^a;\bar{w_i^b}$)转化为聚焦于在任务下的重要信息的向量表示$v_i^a$，这个步骤是通过另一个前馈网络完成：
$$
v_i^a = \rm{MLP}^{pair}([w_i^a;\bar{w_i^b}])\\
v_i^b = \rm{MLP}^{pair}([w_i^b;\bar{w_i^a}])
$$
​	通过看句子中的每一个组成部分的来源，可能知道patient和men在一个方向上是蕴含关系，反之则不成立。

- 第三阶段，将所有局部证据整合成全局决策

  最终，决策网络从词对中聚合数据，并且据此提出决策。

  将向量结果加和并传递到一个MLP分类器中，用于预测两个句子的关系(蕴含，矛盾，中立):
  $$
  v^a = \sum_i v_i^a \\
  v^b = \sum_j v_j^b
  $$

  > 在Parikh等人[2016]的工作中，所有的MLP都有大小为200的两个隐层以及一个ReLU激活函数。
  >
  > 整个过程由同一个计算图捕获，并且网络使用交叉熵损失并进行端对端训练，预训练得到的词向量不会随着剩余网络的变化而变化，并且依赖$\rm{MLP}^{transform}$进行迭代，在写本书的时候，这个结构是SNLI数据集上取得最好效果的网络。



## 12.3总结

本章值得留意的一个新组件是对软对齐权重$\alpha_i^a$(也称为注意力)的使用。将在17章讨论RNN中进行深度探讨。