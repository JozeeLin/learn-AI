# 第十章 预训练的词表示

神经网络方法的一个**主要组成部分是使用词嵌入**----将每个特征表示为低维空间中的向量.但是这些向量来自哪里?本章对常见方法进行了调研.

## 随机初始化

训练词嵌入向量的初始化:将词嵌入看作与其他模型参数一样,将嵌入向量初始化为随机值,然后在使用网络训练将它们调整为"好"的向量.

随机初始化的方式主要有以下几种:

- 在$[-\frac{1}{2d},\frac{1}{2d}]$范围内均匀采样随机数字来初始化词向量,其中d为维度数.

  此方法在高效w2v实现[Mikolov et al.,2013,a]

- 使用xavier初始化(参见5.2.2节),使用从$[-\frac{\sqrt{6}}{\sqrt{d}},\frac{\sqrt{6}}{\sqrt{d}}]$中均匀采样的值来初始化词向量.

## 有监督的特定任务的预训练

如果目标任务A只有数量有限的标注数据(例如:句法分析),但是对于一个辅助任务B(比如词性标注),我们有更多的标注数据,那么我们可以使用辅助任务B的训练集来训练出词向量来提升任务B的预测器性能.

同时我们也可以使用预训练生成的词向量作为任务A的词向量集合,或者在任务A中进一步调整预训练好的词向量.

另外的一种方式是,两个任务共同训练.更多细节在20章中获取.

## 无监督的预训练

常见的情况是,我们没有足够的数量的标注数据的辅助任务.在这种情况下,我们诉诸"无监督"辅助任务,可以对大量未标注的文本进行训练.

无监督方法背后的关键思想是,"相似"词的词嵌入向量是相似的.<u>目前的方法来源于分布假设[harris,1954],表明出现在相似的上下文中的词是相似的.不同的方法都创建了有监督的训练实例,其目标是从上下文预测词,或者根据词预测上下文.</u>

> 在第九章的最后一节中,看到了<u>语言模型如何将词向量作为训练的副产品.</u>
>
> 实际上,语言模型可以被视为一种"无监督"的方法,基于前k个词的上下文来预测词.

**在大量未标注数据上训练词嵌入的一个重要优点是它为未出现的有监督训练集中的词提供向量表示.**

> 理想情况下,这些词的表示将与训练集中出现的相关词相似,从而使模型能够更好的泛化到未知事件.

**重要(数据集对应的知识域):**辅助问题的选择(正在预测的是什么,基于什么样的上下文)对结果向量的影响要比用于训练它们的学习方法来得更加大.

### 使用预训练的词嵌入

使用预训练的词嵌入时应该采取的一些选择:

1. 第一个选择是关于预处理:是否应该使用预先训练的词向量,还是每个向量应该归一化为单位长度?
2. 第二个选择是在这个任务上对预训练的向量进行微调.(具体请看书)

## 词嵌入算法

神经网络社区倾向于从**分布式表示**[Hinton et al., 1987]的角度思考.在分布式表示中,每个实体被表示为值的向量("激活的模式"),并且实体的含义及其实体的关系由向量中的激活以及不同向量之间的相似性来捕获.<u>在语言处理的上下文中,这意味着不应将词(和句子)映射到离散维度,而是映射到共享的低维空间,其中每个单词将与d维向量相关联,词将被其与其他单词的关系和其向量中的激活值所捕获.</u>

自然语言处理社区倾向于从**分布语义**的角度思考,其中一个词的含义可以从其在语料库中的分布中导出,即从其被使用的语境的总和中导出.在相似上下文中出现的词倾向于具有相似的含义.

> 这两种表示词的方法----在复杂算法的**上下文中学习的激活模式以及与其他词或句法结构的共现模式作为度量**.都会产生看似非常不同的单词表示方法,从而导致不同的算法家族和思路.

### 分布式假设和词表示

关于语言和词义的分布式假设表明,在相同上下文中出现的词倾向于具有相似的含义.

> 直觉地,当人们遇到一个未知词的时候,根据它的上下文推断出这个词的含义.这种想法导致了**分布式语义学**领域的产生:一个对"根据词语在大型文本语料库中的分布特性量化语言学术语之间的语义相似性"感兴趣的研究领域.

#### 词-上下文矩阵

上下文矩阵$M_{[i,j]}$为在大语料库中量化得到的词与上下文之间的关联强度.其中每行i表示一个单词,每列j表示词出现处的语言学上下文.

更正式地,定义$V_w$表示一组词(词表)和$V_c$表示可能的上下文集合.那么矩阵$M^f$为词-上下文矩阵,由$M_{[i,j]}^j=f(w_i,c_j)$定义得到,f是词与上下文之间的相关性强度的度量.

#### 相似性度量

通过计算相应向量之间的相似度来计算词之间的相似度.

- 一个常见和有效的措施是<u>余弦相似度</u>,测量向量之间角度的余弦:
  $$
  \rm{sim_{cos}}(u,v) = \frac{u.v}{\|u\|_2\|v\|_2} = \frac{\sum_iu_{[i]}.v_{[i]}}{\sqrt{\sum_i(u_{[i]})^2}\sqrt{\sum_i(v_{[i]})^2}}
  $$

- 另一个流行的度量方式是广义Jaccard相似度,定义如下:
  $$
  \rm{sim_{Jaccard}}(u,v) = \frac{\sum_i \min(u_{[i]},v_{[i]})}{\sum_i \max(u_{[i]},v_{[i]})}
  $$



#### 词-上下文权重和PMI

函数f通常基于来自大规模语料库的计数.使用$\#(w,c)$表示语料库D中词w在上下文c中出现的次数,令|D|等于语料库的大小,直观地定义$f(w,c)=\#(w,c)$,或者$f(w,c)=P(w,c)=\frac{\#(w,c)}{|D|}$.

类似于TF-IDF的算法一样,PMI算法可以捕获到更加有效的信息,而不会受停用词的干扰:
$$
\begin{aligned}
\rm{PMI}(w,c) &= \log \frac{P(w,c)}{P(w)P(c)} \\
&= \log \frac{\#(w,c).|D|}{\#{w}.\#(c)}
\end{aligned}
$$
PMI被广泛应用于词相似度和分布式语义任务上[Dagan et al.,1994, Turney, 2001, Turney and Pantel, 2010]

> 问题:由于$\#(w,c)$有可能不存在,那么PMI(w,c)=log 0 = $-\infty$.
>
> 常见的解决方案是使用正PMI(PPMI)度量,其中所有负值都被0替换:
> $$
> \rm{PPMI(w,c) = max(PMI(w,c),0)}
> $$
> 缺点:它倾向于为稀有时间赋予高值.

#### 通过矩阵因式分解进行降维

将词表示为其出现的上下文的显示集合的潜在障碍是**数据稀疏性**---矩阵M中的某些条目可能不正确,因为我们没有观察到足够的数据点.另外,**显式词向量具有非常高的维度**(取决于上下文的定义).

通过奇异值分解(SVD)的降维技术来考虑数据的低阶表示,可以缓解这两个问题.

SVD通过分解矩阵M为两个窄矩阵来工作,一个是W词矩阵,一个是C上下文矩阵.

### 从神经语言模型到分布式表示

相比于前面提到的基于计数的方法,神经网络社区主张使用分布式来表示词义.

方程(9.3)中的网络设计是语言模型任务驱动的.提出了两个要求:

1. 需要对词生成概率分布
2. 需要以概率链法则组合的上下文为条件来产生句子级概率估计.

产生概率分布需要费时地计算一个包含输出词汇表中所有单词的归一化项(softmax);同时根据链式法则限制,需要将上下文切分为前k元组.

####Collobert和Weston

如果只关心产生的词的表示(副产品),那么两个限制都可以松弛.(参考Collobert和Weston [2008])

1. 将前k元的词的上下文改变为词周围窗口
2. 放弃概率输出要求;模型只是针对每个词打分数,而不是计算给定上下文的目标词概率分布.这消除了昂贵的归一化计算的需要.

**模型描述如下:**

令w为目标词,$c_{1:k}$为上下文的有序列表,$v_w(w)$和$v_c(c)$嵌入函数是将词和上下文索引映射到$d_{emb}$维向量(假设词和上下文向量具有相同数量的维度).

模型通过将嵌入词和上下文拼接成向量x,计算词-上下文对的得分$s(w,c_{1:k})$,该向量被送到具有一个隐层的MLP中,每个输出即为分配给词-上下文组合的分数:
$$
\begin{aligned}
s(w,c_{1:k})&=g(xU).v\\
x &= [v_c(c_1);...;v_c(c_k);v_w(w)]
\end{aligned}
$$
网络基于边缘的排序损失来训练,以至少1的边界来使正确的词-上下文对得分高于不正确的词-上下文对.

给定词-上下文对的损失函数为$L(w,c_{1:k})$:
$$
L(w,c,w^{'}) = \max(0,1-(s(w,c_{1:k})-s(w^{'},c_{1:k})))
$$
这里$w^{'}$是单词表的随机词.

训练过程重复地从语料库中读取词-上下文对,并且对于每个样本中的随机词$w^{'}$使用$w^{'}$计算损失$L(w,c,w^{'})$,并更新参数U,v以及词和上下文词嵌入,以此来最小化损失.

#### Word2Vec

与上面的算法类似,也是基于神经语言模型,修改了模型来产生更快的结果.

textscWord2Vec不是个单一的算法:它是一个软件包,实现两种不同的上下文表示(CBOW和skip-gram)和两个不同的优化目标(负采样和层次softmax).

**这里我们关注负采样目标(NS).**

Word2Vec的NS变体通过训练网络来从"坏"的词-上下文对中区分出"好"的词-上下文对.然而,**W2V用概率目标代替了基于边缘的排序目标**.

考虑正确的词-上下文对集合D,以及不正确的词-上下文对集合$\bar{D}$.该算法的目标是估计来自正确集合D的词-上下文对的概率P(D=1|w,c).<u>对于来自D的对,其得分概率应该高(1),来自$\bar{D}$的对,其得分应该低(0).</u>

概率约束规定P(D=1|w,c)=1-P(D=0|w,c).将概率函数建模为s(w,c):
$$
P(D=1|w,c) = \frac{1}{1+e^{-s(w,c)}}
$$
**算法的语料库目标是最大化数据DU$\bar{D}$的对数似然函数**.
$$
\mathcal{L}(\Theta;D,\bar{D}) = \sum_{(w,c) \in D} \log P(D=1|w,c)+\sum_{(w,c) \in \bar{D}}\log P(D=0|w,c)
$$
**注:**正例集D是从语料库中生成的.负例集$\bar{D}$可以用很多种方法生成.

在W2V中,通过以下步骤**生成负例集:**

> 对于每个好的词-上下文对(w,c)$\in$D,采样k个词$w_{1:k}$,并将$(w_i,c)$中的每一个作为负例加到$\bar{D}$中.这会导致负例集比正例集大k倍.
>
> 负采样中的数k是算法的一个参数.

#### CBOW

除了将目标从基于边缘方式改为概率方式外,W2V还大大简化了词-上下文得分函数s(w,c).

对于多词的上下文$c_{1:k}$,w2v的CBOW变体将上下文向量c定义为上下文组成的词嵌入向量的累计:$c=\sum_{i=1}^k c_i$,然后将得分定义为简单的$s(w,c)=w.c$,从而得到:
$$
P(D=1|w,c_{1:k}) = \frac{1}{1+e^{-(w.c_1+w.c_2+...+w.c_k)}}
$$
**CBOW变体丢失上下文成分之间的顺序信息**.作为回报,它允许使用变长的上下文.

#### skip-gram

w2v变体skip-gram在打分上**分离了上下文成分之间的依赖关系**.

对于k个元素的上下文$c_{1:k}$,**skip-gram变体假设上下文中的元素$c_i$和其他元素相独立**,基本上将它们视为不同的上下文,即一个词-上下文对$(w,c_{1:k})$在语料D中表示成不同的k个上下文:$(w,c_1),...,(w,c_k)$.得分函数S(w,c)的定义和CBOW一样，但是现在每个上下文是一个单独的词嵌入向量：
$$
P(D=1|w,c_i) = \frac{1}{1+e^{-w.c_i}} \\
P(D=1|w,c_{1:k}) = \prod_{i=1}^k P(D=1|w,c_i) = \prod_{i=1}^k \frac{1}{1+e^{-w.c_i}}\\
\log P(D=1|w,c_{1:k}) = \sum_{i=1}^k \log \frac{1}{1+e^{-w.c_i}}
$$
虽然在上下文元素间引入了强大的**独立性假设**，但是skip-gram变体在实践中非常有效，并且非常常用。



### 词语联系

**分布式“基于计数”的方法和分布式“神经”方法都是基于分布假设，尝试基于他们出现的上下文之间的相似性来捕获词之间的相似性。**

w2v模型训练会产生两个词嵌入矩阵：

- 目标词嵌入矩阵（保留）

- 上下文矩阵（丢弃）



**两种方法的联系：**

基于假设负例是根据词在语料库中的频率$P(w)=\frac{\#(w)}{\sum_{w^{'}}\#(w^{'})}$来采样的。

Levy和Goldberg表明，将skip-gram的上下文和k个负样本的负采样目标相组合，通过设置$w.c = M^{'}_{[w,c]} = \rm{PMI}(w,c)-\log k$进行全局目标最小化。也就是说，Word2vec是隐含地分解了与词-上下文PMI矩阵密切相关的矩阵$M^{'}$!值得注意的是，w2v并没有明确地构造矩阵$M^{'}$。

> Word2Vec的SGNS变体与词-上下文矩阵分解的关联将神经方法和传统的“基于计数”方法联系在一起，这表明在“分布式”表示研究中吸取的教训可以迁移到“分布式”算法上，反之亦然，而在更深层的意义上，两个算法家族是对等的。



### 其他算法

#### NCE

> Mnih和Kavukcuoglu[2013]

模型为：
$$
P(D=1|w,c_i) = \frac{e^{-w.c_i}}{e^{-w.c_i}+k×q(w)} \\
P(D=0|w,c_i) = \frac{k×q(w)}{e^{-w.c_i}+k×q(w)}
$$
这里$q(w) = \frac{\#(w)}{|D|}$是语料库中被观察到的词w的一元组频率。

该算法基于噪声对比概率估计建模技术[Gutmann and Hyvarinen, 2010].根据Levy和Goldberg[2014]的模型，这个目标相当于将矩阵项为对数条件概率$\log P(w|c) - \log k$的词-上下文矩阵进行因式分解。

#### Glove

Glove算法构建了一个显示的词-上下文矩阵，并且训练了词和上下文向量w和c，并试图满足：
$$
w.c + b_{[w]} + b_{[c]} = \log \#(w,c)
$$
这里$b_{[w]},b_{[c]}$是特定词和特定上下文的训练偏置。

**就矩阵分解而言**，如果固定$b_{[w]}=\log \#(w)$和$b_{[c]} = \log \#(c)$，我们将会得到一个非常类似于将词-上下文PMI矩阵分解的目标。

**优化目标是加权最小二乘法损失函数，给频繁词条正确地赋予更多权重**。

## 上下文的选择

**上下文的选择对词向量结果及其编码的相似性具有很大的影响。**

### 窗口方法

常见的方法是滑动窗口方法，通过观测序列2m+1个单词来生成辅助任务，中间的词为目标词，每侧的m个词称为上下文，接下来:

- 要么创建单个任务，目标是基于所有上下文词（表示方法用CBOW或向量拼接）来预测目标词
- 要么创建2m个不同的任务，每个任务将目标词与不同的上下文词进行配对。（skip-gram）

#### 窗口大小的影响

滑动窗口的大小对向量相似度结果有很大的影响。

**窗口较大易于产生更大的主题相似性，而较小的窗口易于产生更多的功能和句法相似性**

#### 窗口位置

- 使用CBOW和skip-gram上下文表示时，窗口中的所有不同的上下文同等重要。
- 通过使用位置上下文可以很简单地将这种信息包括进去：指示每个上下文词即是其与焦点词的相对位（即上下文词是“the：+2”而不是“the”，表示词出现在焦点词右侧两个位置）。

> ling等[2015a]表明位置向量要比窗口向量更优越，当它们被用于初始化词性标注和依存句法分析网络时。

#### 变体

许多基于窗口方法的变化方案是可行的。

- 一些变化方案的改变是在数据预处理阶段，如进行词汇还原，文本归一化，过滤过短或过长的句子，或去除大小写（例如，预处理步骤可参见dos santos和gatti[2014]）。

- 另外一些变体方案是对部分语料库进行子采样，以一定的概率从带有常见或稀有焦点词的窗口中创建任务。每轮可以使用不同的窗口大小。
- 一些改变可能对窗口中的不同位置进行权衡，更多侧重于距离目标词更近的上下文词而不是距离较远的上下文词。

> Word2Vec实现的强大性能很大程度上得益于对这些超参数设定了很好的默认值。Levy等人[2015]详细讨论了其中的部分超参数。

### 句子，段落或文档

skip-gram或CBOW可以把一个句子，段落或文档视为一个词的上下文。相当于使用非常大的窗口。

**期望得到的是主题相似的词向量。**

### 句法窗口

使用句法替代句子中的线性上下文[bansal et al.,2014, Levy and Gold-berg, 2014]。使用依存句法解析器自动解析文本，并将词的上下文视为解析树中邻近的词以及与之相关的句法关系。

### 多语种

机器翻译中，使用对齐文本进行训练。

### 基于字符级别的子词的表示

（略）

## 处理多字单元和字变形

（略）

## 分布式方法的限制

### 相似性的定义

有时候想知道的是主题的相似性，有时候却希望是功能性的相似性

### 害群之马

语言中存在偏见。

### 反义词

分布式方法认为反义词之间也存在相似性

### 语料库的偏好

语料库对应的领域或者偏好会影响模型的性能。

### 语境缺乏

（略）