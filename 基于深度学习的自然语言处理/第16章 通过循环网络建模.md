# 第16章 通过循环网络建模

现在探索RNN在NLP应用中的一些实例。

## 16.1 接收器

RNN最简单的应用就是作为一个接收器：读入一个序列，最后产生一个二值或者多分类的结果。

> 在很多NLP任务中，词语的顺序以及句子的结构并非非常重要，所以**词袋模型以及n元文法袋模型就能够得到和RNN接近甚至更好的性能。**

本节展示了两个在语言问题中使用接收器的例子：

1. 情感分类。
2. 人为构造（展示了RNN强大的序列学习能力）

### 16.1.1 情感分类器

**句子级情感分类**

给定一个句子(通常是评论信息)，需要将这个句子分成两类中的一个：消极的或积极的。(还有一种更复杂的分类:消极，中立，积极)。

实际的情感分类不仅需要理解单独的短语，还需要理解这些短语出现的上下文，从语言角度整体构建句子的情感极性，同时还需要很好地处理讽刺和隐喻。（更加详细了解请参考Pang和Lee【2008】）

**建模过程：**

- RNN读取序列化后的词语
- RNN的最终状态送入一个MLP，MLP通过softmax层输出二分类的值。

此训练过程为监督学习，使用交叉熵损失。

更细粒度的分类任务，需要将感情极性给出一个1\~5或者1\~10的值,那么就需要把softmax层改成5分类或者10分类。
$$
\rm{p(label=k|w_{1:n})=\hat{y}_{[k]}} \\
\hat{y} = \rm{softmax(MLP(RNN(x_{1:n})))} \tag{16.1}\\
x_{1:n} = E_{[w_1]},...,E_{[w_n]}
$$
词嵌入矩阵E通过预训练的词向量初始化。

**改进版(1)**

通常将式(16.1)扩展为2个RNN的结构，一个以给出的顺序读入句子序列，另一个则以逆序读入句子序列。这两个RNN的最终状态拼接起来送入MLP分类器中：
$$
\rm{p(label=k|w_{1:n}) = \hat{y}_{[k]}} \\
\hat{y} = \rm{softmax(MLP([RNN^f(x_{1:n});RNN^b(x_{n:1})]))} \\ \tag{16.2}
x_{1:n} = E_{[w_1]},...,E_{[w_n]}
$$
**改进版(2)**

对于更长的句子，Li等人[2015]发现使用层次化的结构很有帮助，句子在这种层次化的结构中以标点符号为分隔板拆分成了更短长度的单元。

然后，每个拆分后的句子单位被送入式(16.2)描述的RNN中。

输出向量的序列(每个句子单位一个向量)随后被送入一个式(16.1)描述的RNN接收器中。

形式地，给定一个句子$w_{1:n}$，这个句子被分为m个单位$w^1_{1:l_1},...,w^m_{1:l_m}$，这个结构由下式给出：
$$
\rm{p(label=k|w_{1:n}) = \hat{y}_{[k]}}\\
\rm{\hat{y} = softmax(MLP(RNN(z_{1:m})))}\\
\rm{z_i = [RNN^f(x^i_{1:l_i});RNN^b(x^i_{l_i:1})]} \\ \tag{16.3}
\rm{x^i_{1:l_i}} = E_{[w^i_1],...,E_{w^i_{l_i}}}
$$
不同的m个单位得到不同的情感标签。更高一层的接收器读入低层编码器产生的摘要信息$z_{1:m}$，并决定总体的情感分类结果。



**文本级的情感分类**

Tang等人[2015]发现这个任务很适合使用类似Li等人[2015]的层次化结构的方法。[式(16.3)]:

每个句子$s_i$通过一个门结构的RNN编码得到一个向量$z_i$，然后将向量$z_{1:n}$送入第2个门结构的RNN中，得到一个向量$h=RNN(z_{1:n})$，这个向量用来预测:$\rm{\hat{y} = softmax(MLP(h))}$

**改进版(1)**

所有来自文本级RNN的中间向量都被保留，它们的平均值被送入一个MLP中

($\rm{h_{1:n}=RNN^*(z_{1:n}), \hat{y}= softmax(MLP(\frac{1}{n} \sum_{i=1}^n h_i))}$)

这种使用平均值的效果更胜一筹。

### 16.1.2 主谓一致语法检查

英语句子的规则:现在时态谓语动词的单复数必须与动词的主语一致。

**难点：**主语和谓语可能被任意长度的语言单位隔开，这就导致了中间可能出现于主语单复数不一样的名词。

**措施：**给出句子的正确句法分析树，动词与其主语之间的关系变得显而易见。

模型直接作为一种接收器被训练：
$$
\rm{\hat{y} = softmax(MLP(RNN(E_{[w_1]},...,E_{[w_n]})))}
$$
使用交叉熵损失函数。

**这是一项困难的任务**。非常间接的监督：监督信号不包括任何找到语法信息的线索。

RNN必须学习**数字的概念**(复数和单数词属于不同的组)、**主谓一致的概念**(动词的形式应该符合主语的形式)以及**主体的概念**(以确定动词之前的哪个名词决定动词的形式)。识别正确的主语需要**学习识别嵌套结构的语法标记**。

## 16.2作为特征提取器的RNN

RNN的一个最主要的应用场景就是作为灵活可训练的特征提取器，在处理序列问题时能够替代传统的特征提取通道。**特别地，RNN是基于窗口的特征提取器的良好替代者**。

### 16.2.1 词性标注

**框架：深度双向RNN** 词性标注是序列标注任务的特征情况，为句子中的每一个词语分配词性标签。

- 给定一个含有n个词语的句子$s=w_{1:n}$，使用一个特征提取函数$x_i = \phi(s,i)$来把句子转化为输入向量$x_{1:n}$。

- 输入向量将会被送入一个深度双向RNN中，产生一个输出向量$y_{1:n} = biRNN*(x_{1:n})$。

- 每个向量$y_i$将被送入一个MLP中，用于从可能的k个标签中预测这个词语的标签。

**通过字符级的RNN将词语转化为输入** 如何把一个词语$w_i$转换为一个输入向量$x_i$?

1. 使用词嵌入向量，存在词表覆盖范围的问题OOV
2. 通过词语的字符序列来表示。它可以提供词语的前缀、后缀、拼写提示(大小写，连字符，数字)，词的歧义类的有效提示。

在这个任务中，使用两个字符级的RNN。

- 对于一个由字符$c_1,...,c_l$组成的单词w，我们将每一个字符映射到一个对应的嵌入向量$\textbf{c}_i$。

- 然后，基于这些字符通过一个前向RNN和一个逆向RNN将单词编码。

RNN的结果可以替代词向量，或者以一种更好的方式，拼接在词向量后面：
$$
\rm{x_i = \phi(s,i) = [E_{[w_i]};RNN^f(c_{1:l});RNN^b(c_{l:1})]}
$$
**最终的模型** 标注模型最后变成了：
$$
\begin{aligned}
p(t_i=j|w_1,...,w_n) &= \rm{softmax(MLP(biRNN(x_{1:n},i)))_{[j]}}\\
x_i &= \phi(s,i) = \rm{[E_{[w_i]};RNN^f(c_{1:l});RNN^b(c_{l:1})]}

\end{aligned}
$$
**字符级的卷积和池化** 使用字符级的卷积和池化神经网络来表示词语

Ma和Hovy[2016]展示了对每个字符使用窗口大小k=3的一层卷积-池化网络实际上对于词性标注任务和命名实体识别任务是很有效的。

**结构化的模型** 

上述的模型中，单词i的标签预测时独立于其他标签预测来计算的。但是第i个标签可能会依赖模型前面预测出的一些标签。

- 这个依赖关系可以是依赖确定的前k个标签(遵循马尔科夫假设)，在这种情况下我们使用标签的嵌入形式$E_{[t]}$，结果就是：

$$
\rm{p(t_i=j|w_1,...,w_n,t_{i-1},...,t_{i-k})} = softmax(MLP([biRNN(x_{1:n},i);E_{t_{i-1}};...;E_{[t_{i-k}]}]))_{[j]}
$$

- 依赖于整个序列中第i个标签之前的预测结果$t_{1:i-1}$，在这种情况下，使用一个RNN来编码整个标签序列：
  $$
  \rm{p(t_i=j|w_1,...,w_n,t_{1:i-1})} = softmax(MLP([biRNN(x_{1:n},i);RNN^t(t_{1:i-1})))_{[j]}
  $$




以上这些模型被用于Vaswani等[2016]提出的CCG-supertagging任务中。

### 16.2.2 RNN-CNN 文本分类

在字符上应用一个层次化的卷积-池化网络(13.3节)，目的是得到一个更短的向量序列，这个向量序列代表了一种字符级别之上的单位(可以是大于或小于一个词语的单位)，然后把结果的向量序列送入2个RNN以及分类层中。

> Xiao和Cho[2016]在数个文本分类任务中的模型:
>
> - 一个窗口大小为k的卷积核应用于输入向量的序列
> - 对任意两组相邻的输出向量进行max-pooling操作，使序列长度减半。
> - 使用dropout机制
> - 随后产生的输出向量被送入一个双向的GRU RNN里
> - 再次使用dropout机制
> - 最后送入一个分类器中(一个全连接层后跟一个softmax层)



### 16.2.3 弧分解依存句法分析

依存句法分析任务的弧分解(arc-factored)方法[7.7节]：

给定了有n个词$w_{1:n}$的句子以及对应的词性标签$t_{1:n}$，然后需要为每一对词$w_i,w_j$赋予一个分数，表示词语$w_i$是词语$w_j$的核心词的可能性。

在8.6节我们为这个任务导出了一个复杂的特征函数。这里我们可以用两个双向RNN的向量拼接所代替：

给定了词语$w_{1:n}$和词性标签$t_{1:n}$以及对应的特征向量$\textbf{w}_{1:n}$和$\textbf{t}_{1:n}$，通过一个双向RNN得到编码$\textbf{v}_i$，这个编码对句子中的每一个位置上的词向量和对应的词性标签向量的拼接结果送入深度双向RNN的编码:
$$
\textbf{v}_{1:n} = \rm{biRNN}^*(\textbf{x}_{1:n})\\
\textbf{x}_i = [\textbf{w}_i;\textbf{t}_i]
$$
然后，将双向RNN的输出向量拼接送入MLP的方式，得到对一个核心词-修饰词候选对的打分：
$$
\rm{A_{RC}S_{CORE}(\textbf{h},\textbf{m},w_{1:n},t_{1:n}) = MLP(\phi(\textbf{h,m,s})) = MLP([\textbf{v}_h;\textbf{v}_m])}
$$
**总结：**如果一个任务对**词语顺序**或者**句子结构**敏感，并且这个任务使用词语作为特征，那么这个任务中的词语就能够**被使用词语自身训练的双向LSTM输出的词向量代替**。【Kiperwasser和Goldberg[2016b]以及Cross和Huang[2016a,b]的关于基于转移句法分析任务中都有使用，并得到了令人印象深刻的结果。】

